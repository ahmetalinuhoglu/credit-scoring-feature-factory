{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Model Development Pipeline\n",
    "\n",
    "Interactive notebook for step-by-step model development with the refactored pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Config loading and data splitting\n",
    "2. Constant feature elimination\n",
    "3. Missing value elimination\n",
    "4. IV (Information Value) filtering\n",
    "5. PSI stability filtering\n",
    "6. Correlation elimination\n",
    "7. Forward feature selection (XGBoost)\n",
    "8. Model evaluation (Train / Test / OOT quarterly)\n",
    "9. Validation checks\n",
    "10. Report generation\n",
    "\n",
    "Each cell is self-contained and re-runnable. You can also run the full pipeline in one call at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve project root regardless of where the notebook is run from\n",
    "project_root = str(Path.cwd().parent) if Path.cwd().name == \"notebooks\" else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-5s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "from src.config.loader import load_config\n",
    "from src.components import (\n",
    "    DataSplitter,\n",
    "    ConstantFilter,\n",
    "    MissingFilter,\n",
    "    IVFilter,\n",
    "    PSIFilter,\n",
    "    CorrelationFilter,\n",
    "    ForwardFeatureSelector,\n",
    "    ModelEvaluator,\n",
    ")\n",
    "from src.io.output_manager import OutputManager\n",
    "from src.pipeline.orchestrator import PipelineOrchestrator\n",
    "from src.validation.data_checks import DataValidator\n",
    "from src.validation.model_checks import ModelValidator\n",
    "from src.reporting.report_exporter import ReportExporter\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config from YAML (edit the YAML or use overrides here)\n",
    "config = load_config(\"config/model_development.yaml\")\n",
    "\n",
    "# Override for this session (uncomment and edit as needed):\n",
    "# config = load_config(\"config/model_development.yaml\", overrides={\n",
    "#     \"splitting\": {\"train_end_date\": \"2024-06-30\"},\n",
    "#     \"steps\": {\"iv\": {\"min_iv\": 0.03}},\n",
    "# })\n",
    "\n",
    "print(f\"Input:          {config.data.input_path}\")\n",
    "print(f\"Train end date: {config.splitting.train_end_date}\")\n",
    "print(f\"Target:         {config.data.target_column}\")\n",
    "print(f\"IV range:       [{config.steps.iv.min_iv}, {config.steps.iv.max_iv}]\")\n",
    "print(f\"PSI threshold:  {config.steps.psi.threshold}\")\n",
    "print(f\"Corr threshold: {config.steps.correlation.threshold}\")\n",
    "print(f\"AUC threshold:  {config.steps.selection.auc_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Splitting\n",
    "\n",
    "Load the feature parquet and split into Train / Test / OOT quarters by `train_end_date`.\n",
    "Test set is a stratified hold-out from the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(config.data.input_path)\n",
    "print(f\"Loaded {len(df):,} rows x {len(df.columns)} columns\")\n",
    "\n",
    "splitter = DataSplitter(\n",
    "    data_config=config.data,\n",
    "    splitting_config=config.splitting,\n",
    "    seed=config.reproducibility.global_seed,\n",
    ")\n",
    "split_result = splitter.split(df)\n",
    "\n",
    "train_df = split_result.train\n",
    "test_df = split_result.test\n",
    "oot_quarters = split_result.oot_quarters\n",
    "feature_columns = split_result.feature_columns\n",
    "\n",
    "target = config.data.target_column\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df[target]\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_columns)}\")\n",
    "print(f\"Train: {len(train_df):,} rows, bad rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test:  {len(test_df):,} rows, bad rate: {y_test.mean():.2%}\")\n",
    "for label, qdf in sorted(oot_quarters.items()):\n",
    "    print(f\"OOT {label}: {len(qdf):,} rows, bad rate: {qdf[target].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Steps\n",
    "\n",
    "Run each elimination / selection step individually. Each step takes the current feature set,\n",
    "removes features that fail the criteria, and returns a `StepResult` with details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Constant Filter\n",
    "# Removes features with fewer than min_unique_values distinct values.\n",
    "\n",
    "constant_filter = ConstantFilter(config.steps.constant)\n",
    "constant_result = constant_filter.fit(X_train[feature_columns], y_train)\n",
    "feature_columns = constant_result.output_features\n",
    "\n",
    "print(constant_result.summary())\n",
    "if constant_result.eliminated_features:\n",
    "    print(f\"\\nEliminated: {constant_result.eliminated_features[:10]}\")\n",
    "    if len(constant_result.eliminated_features) > 10:\n",
    "        print(f\"  ... and {len(constant_result.eliminated_features) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Missing Filter\n",
    "# Removes features with missing rate above the threshold.\n",
    "\n",
    "missing_filter = MissingFilter(config.steps.missing)\n",
    "missing_result = missing_filter.fit(X_train[feature_columns], y_train)\n",
    "feature_columns = missing_result.output_features\n",
    "\n",
    "print(missing_result.summary())\n",
    "if missing_result.eliminated_features:\n",
    "    print(f\"\\nEliminated: {missing_result.eliminated_features[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: IV Filter\n",
    "# Removes features with IV below min_iv (useless) or above max_iv (suspicious).\n",
    "\n",
    "iv_filter = IVFilter(config.steps.iv)\n",
    "iv_result = iv_filter.fit(X_train[feature_columns], y_train)\n",
    "feature_columns = iv_result.output_features\n",
    "\n",
    "print(iv_result.summary())\n",
    "\n",
    "# Show top features by IV\n",
    "if not iv_result.results_df.empty:\n",
    "    kept = iv_result.results_df[\n",
    "        iv_result.results_df[\"feature\"].isin(feature_columns)\n",
    "    ].sort_values(\"iv\", ascending=False)\n",
    "    print(f\"\\nTop 10 features by IV:\")\n",
    "    display(kept.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: PSI Filter\n",
    "# Removes features with unstable distributions (within training data).\n",
    "\n",
    "psi_filter = PSIFilter(config.steps.psi)\n",
    "psi_result = psi_filter.fit(\n",
    "    X_train[feature_columns], y_train,\n",
    "    train_dates=train_df[config.data.date_column],\n",
    ")\n",
    "feature_columns = psi_result.output_features\n",
    "\n",
    "print(psi_result.summary())\n",
    "if psi_result.eliminated_features:\n",
    "    print(f\"\\nEliminated (unstable): {psi_result.eliminated_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Correlation Filter\n",
    "# Greedy removal: among correlated pairs (|r| > threshold), drop the lower-IV feature.\n",
    "\n",
    "correlation_filter = CorrelationFilter(config.steps.correlation)\n",
    "corr_result = correlation_filter.fit(X_train[feature_columns], y_train)\n",
    "feature_columns = corr_result.output_features\n",
    "\n",
    "print(corr_result.summary())\n",
    "if corr_result.eliminated_features:\n",
    "    print(f\"\\nEliminated (correlated): {corr_result.eliminated_features[:10]}\")\n",
    "    if len(corr_result.eliminated_features) > 10:\n",
    "        print(f\"  ... and {len(corr_result.eliminated_features) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Forward Feature Selection\n",
    "# Sequentially adds features (ordered by IV) that improve test AUC above threshold.\n",
    "\n",
    "selector = ForwardFeatureSelector(config.steps.selection, config.model)\n",
    "selection_result = selector.fit(\n",
    "    X_train[feature_columns], y_train,\n",
    "    X_test=X_test[feature_columns], y_test=y_test,\n",
    ")\n",
    "selected_features = selection_result.output_features\n",
    "\n",
    "print(selection_result.summary())\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "if not selection_result.results_df.empty:\n",
    "    print(\"\\nSelection details:\")\n",
    "    display(selection_result.results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Evaluation\n",
    "# Train final model on selected features, evaluate on Train/Test/OOT.\n",
    "\n",
    "evaluator = ModelEvaluator(config.model, config.evaluation)\n",
    "eval_result = evaluator.fit(\n",
    "    X_train[selected_features], y_train,\n",
    "    X_test=X_test[selected_features], y_test=y_test,\n",
    "    oot_quarters={label: (qdf[selected_features], qdf[target]) for label, qdf in oot_quarters.items()},\n",
    ")\n",
    "\n",
    "print(eval_result.summary())\n",
    "\n",
    "# Display performance table\n",
    "if not eval_result.results_df.empty:\n",
    "    print(\"\\nPerformance by period:\")\n",
    "    display(eval_result.results_df)\n",
    "\n",
    "# Get the trained model for later use\n",
    "final_model = eval_result.metadata.get(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks\n",
    "\n",
    "Run automated model quality checks: overfit gap, OOT degradation, score PSI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation (pre-pipeline checks)\n",
    "data_validator = DataValidator(config)\n",
    "data_report = data_validator.validate(df)\n",
    "\n",
    "print(\"Data Validation Results:\")\n",
    "for check in data_report.checks:\n",
    "    status_icon = \"PASS\" if check.status.value == \"PASS\" else \"FAIL\" if check.status.value == \"FAIL\" else \"WARN\"\n",
    "    print(f\"  [{status_icon}] {check.check_name}: {check.message}\")\n",
    "\n",
    "if data_report.has_critical_failures:\n",
    "    print(\"\\nCRITICAL: Data validation has failures. Review before proceeding.\")\n",
    "\n",
    "# Model validation (post-pipeline checks)\n",
    "model_validator = ModelValidator(config)\n",
    "model_report = model_validator.validate(\n",
    "    eval_result=eval_result,\n",
    "    selected_features=selected_features,\n",
    ")\n",
    "\n",
    "print(\"\\nModel Validation Results:\")\n",
    "for check in model_report.checks:\n",
    "    status_icon = \"PASS\" if check.status.value == \"PASS\" else \"FAIL\" if check.status.value == \"FAIL\" else \"WARN\"\n",
    "    print(f\"  [{status_icon}] {check.check_name}: {check.message}\")\n",
    "    if check.recommendation:\n",
    "        print(f\"         Recommendation: {check.recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything: generate Excel report and persist artifacts\n",
    "output_manager = OutputManager(config)\n",
    "\n",
    "# Save config snapshot\n",
    "output_manager.save_config_snapshot(config)\n",
    "\n",
    "# Collect all step results\n",
    "all_step_results = [\n",
    "    constant_result,\n",
    "    missing_result,\n",
    "    iv_result,\n",
    "    psi_result,\n",
    "    corr_result,\n",
    "    selection_result,\n",
    "    eval_result,\n",
    "]\n",
    "\n",
    "# Save step results\n",
    "step_names = [\n",
    "    \"01_constant\", \"02_missing\", \"03_iv\", \"04_psi\",\n",
    "    \"05_correlation\", \"06_selection\", \"07_evaluation\",\n",
    "]\n",
    "for step_name, result in zip(step_names, all_step_results):\n",
    "    output_manager.save_step_results(step_name, {\n",
    "        \"results\": result.results_df,\n",
    "        \"output_features\": result.output_features,\n",
    "        \"eliminated_features\": result.eliminated_features,\n",
    "    })\n",
    "\n",
    "# Generate Excel report\n",
    "reporter = ReportExporter(config)\n",
    "excel_path = reporter.generate(\n",
    "    step_results=all_step_results,\n",
    "    split_result=split_result,\n",
    "    output_dir=str(output_manager.run_dir / \"reports\"),\n",
    ")\n",
    "\n",
    "output_manager.mark_complete(\"success\")\n",
    "output_manager.save_run_metadata()\n",
    "\n",
    "print(f\"Run ID: {output_manager.run_id}\")\n",
    "print(f\"Output: {output_manager.run_dir}\")\n",
    "print(f\"Report: {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracking\n",
    "\n",
    "Log this run to the experiment tracker for future comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tracking import ExperimentTracker\n",
    "\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Extract metrics from evaluation result\n",
    "perf_df = eval_result.results_df\n",
    "metrics = {\n",
    "    \"n_features_selected\": len(selected_features),\n",
    "}\n",
    "\n",
    "# Extract AUC values if available in results\n",
    "if not perf_df.empty and \"auc\" in perf_df.columns:\n",
    "    for _, row in perf_df.iterrows():\n",
    "        period = row.get(\"period\", \"\")\n",
    "        if \"train\" in str(period).lower():\n",
    "            metrics[\"train_auc\"] = row[\"auc\"]\n",
    "        elif \"test\" in str(period).lower():\n",
    "            metrics[\"test_auc\"] = row[\"auc\"]\n",
    "    # OOT mean AUC\n",
    "    oot_rows = perf_df[perf_df[\"period\"].str.contains(\"oot\", case=False, na=False)]\n",
    "    if not oot_rows.empty:\n",
    "        metrics[\"oot_mean_auc\"] = oot_rows[\"auc\"].mean()\n",
    "\n",
    "tracker.log_run(\n",
    "    run_id=output_manager.run_id,\n",
    "    config=config,\n",
    "    metrics=metrics,\n",
    "    duration=sum(r.duration_seconds for r in all_step_results),\n",
    "    notes=\"Interactive notebook run\",\n",
    ")\n",
    "\n",
    "print(\"Run logged. History:\")\n",
    "display(tracker.get_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative: Full Pipeline Mode\n",
    "\n",
    "Instead of running each step manually above, run the entire pipeline in one call.\n",
    "This uses the `PipelineOrchestrator` which handles step ordering, logging, and artifact saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline in one call (uncomment to use)\n",
    "\n",
    "# config = load_config(\"config/model_development.yaml\")\n",
    "# df = pd.read_parquet(config.data.input_path)\n",
    "#\n",
    "# output_manager = OutputManager(config)\n",
    "# pipeline = PipelineOrchestrator(config, output_manager)\n",
    "#\n",
    "# # Register steps\n",
    "# pipeline.register_step(ConstantFilter(config.steps.constant))\n",
    "# pipeline.register_step(MissingFilter(config.steps.missing))\n",
    "# pipeline.register_step(IVFilter(config.steps.iv))\n",
    "# pipeline.register_step(PSIFilter(config.steps.psi))\n",
    "# pipeline.register_step(CorrelationFilter(config.steps.correlation))\n",
    "# pipeline.register_step(ForwardFeatureSelector(config.steps.selection, config.model))\n",
    "# pipeline.register_step(ModelEvaluator(config.model, config.evaluation))\n",
    "#\n",
    "# results = pipeline.run_all(df)\n",
    "#\n",
    "# print(results.summary())\n",
    "# print(f\"\\nFinal features: {results.final_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Comparison\n",
    "\n",
    "Compare multiple runs from the experiment log or output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tracking import RunComparison\n",
    "\n",
    "comparison = RunComparison()\n",
    "available_runs = comparison.list_runs()\n",
    "print(f\"Available runs ({len(available_runs)}):\")\n",
    "for run_id in available_runs[-5:]:\n",
    "    print(f\"  {run_id}\")\n",
    "\n",
    "# Compare the two most recent runs (uncomment when you have multiple runs)\n",
    "# if len(available_runs) >= 2:\n",
    "#     diff_df = comparison.diff_configs(available_runs[-2], available_runs[-1])\n",
    "#     print(\"\\nConfig differences:\")\n",
    "#     display(diff_df)\n",
    "#\n",
    "#     compare_df = comparison.compare(available_runs[-2:])\n",
    "#     print(\"\\nMetrics comparison:\")\n",
    "#     display(compare_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
