{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Model Development Pipeline\n",
    "\n",
    "Interactive notebook for step-by-step model development. Uses the same code paths as `scripts/run_model_development.py`.\n",
    "\n",
    "**Steps:**\n",
    "1. Load config and data, split into Train / Test / OOT\n",
    "2. Data quality checks\n",
    "3. Constant feature elimination\n",
    "4. Missing value elimination\n",
    "5. IV (Information Value) filtering\n",
    "6. PSI stability filtering\n",
    "7. Correlation elimination\n",
    "8. Sequential feature selection (forward/backward with CV)\n",
    "9. VIF multicollinearity check\n",
    "10. Hyperparameter tuning (Optuna)\n",
    "11. Model evaluation (quarterly metrics, lift tables, importance)\n",
    "12. Score PSI, Bootstrap CI, Calibration, SHAP, Validation\n",
    "13. Generate Excel report and save outputs\n",
    "\n",
    "Each cell is self-contained and re-runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/ahmetalinuhoglu/Documents/Personal/Projects/an-model-development\n",
      "Working dir:  /Users/ahmetalinuhoglu/Documents/Personal/Projects/an-model-development\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "project_root = str(Path.cwd().parent) if Path.cwd().name == \"notebooks\" else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-5s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "from src.config.loader import load_config, save_config\n",
    "from src.config.schema import PipelineConfig\n",
    "from src.io.output_manager import OutputManager\n",
    "from src.model_development.data_loader import load_and_split\n",
    "from src.model_development.eliminators import (\n",
    "    ConstantEliminator,\n",
    "    MissingEliminator,\n",
    "    IVEliminator,\n",
    "    PSIEliminator,\n",
    "    CorrelationEliminator,\n",
    "    VIFEliminator,\n",
    "    QuarterlyPSICheck,\n",
    "    YearlyPSICheck,\n",
    "    ConsecutiveQuarterPSICheck,\n",
    "    HalfSplitPSICheck,\n",
    "    DateSplitPSICheck,\n",
    ")\n",
    "from src.model_development.feature_selector import sequential_feature_selection\n",
    "from src.model_development.hyperparameter_tuner import tune_hyperparameters\n",
    "from src.model_development.evaluator import (\n",
    "    evaluate_model_quarterly,\n",
    "    bootstrap_auc_ci,\n",
    "    compute_score_psi,\n",
    ")\n",
    "from src.model_development import excel_reporter\n",
    "from src.validation.data_checks import DataValidator\n",
    "from src.validation.model_checks import ModelValidator\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working dir:  {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Config\n",
    "\n",
    "Single YAML config drives everything. Override values for this session without editing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:44 | INFO  | Loaded config from config/model_development.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:          data/sample/sample_features.parquet\n",
      "Train end date: 2024-06-30\n",
      "Target:         target\n",
      "Seed:           42\n",
      "IV range:       [0.02, 0.5]\n",
      "PSI threshold:  0.25\n",
      "Corr threshold: 0.8\n",
      "Selection:      forward (max 20)\n",
      "VIF:            enabled (threshold 5.0)\n",
      "Tuning:         enabled (100 trials)\n",
      "Calibration:    enabled (platt)\n",
      "SHAP:           enabled\n",
      "Bootstrap CI:   enabled\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"config/model_development.yaml\")\n",
    "\n",
    "# Override for this session (uncomment and edit as needed):\n",
    "# config = load_config(\"config/model_development.yaml\", cli_overrides={\n",
    "#     \"splitting.train_end_date\": \"2024-06-30\",\n",
    "#     \"steps.iv.min_iv\": 0.03,\n",
    "#     \"model.params.max_depth\": 4,\n",
    "# })\n",
    "\n",
    "print(f\"Input:          {config.data.input_path}\")\n",
    "print(f\"Train end date: {config.splitting.train_end_date}\")\n",
    "print(f\"Target:         {config.data.target_column}\")\n",
    "print(f\"Seed:           {config.reproducibility.global_seed}\")\n",
    "print(f\"IV range:       [{config.steps.iv.min_iv}, {config.steps.iv.max_iv}]\")\n",
    "print(f\"PSI threshold:  {config.steps.psi.threshold}\")\n",
    "print(f\"Corr threshold: {config.steps.correlation.threshold}\")\n",
    "print(f\"Selection:      {config.steps.selection.method} (max {config.steps.selection.max_features})\")\n",
    "print(f\"VIF:            {'enabled' if config.steps.vif.enabled else 'disabled'} (threshold {config.steps.vif.threshold})\")\n",
    "print(f\"Tuning:         {'enabled' if config.model.tuning.enabled else 'disabled'} ({config.model.tuning.n_trials} trials)\")\n",
    "print(f\"Calibration:    {'enabled' if config.evaluation.calibration.enabled else 'disabled'} ({config.evaluation.calibration.method})\")\n",
    "print(f\"SHAP:           {'enabled' if config.evaluation.shap.enabled else 'disabled'}\")\n",
    "print(f\"Bootstrap CI:   {'enabled' if config.evaluation.bootstrap.enabled else 'disabled'}\")\n",
    "\n",
    "# Set seeds\n",
    "seed = config.reproducibility.global_seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data & Split\n",
    "\n",
    "Uses `load_and_split()` — the same function called by the script pipeline.\n",
    "Stratified random or temporal train/test split within training period. OOT auto-split by quarter after `train_end_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:44 | INFO  | Loading data from data/sample/sample_features.parquet\n",
      "03:53:44 | INFO  | Loaded 11,529 rows, 948 columns\n",
      "03:53:44 | INFO  | Train end date: 2024-06-30\n",
      "03:53:44 | INFO  | Training period: 9,342 rows (2022-01-01 to 2024-06-30)\n",
      "03:53:44 | INFO  | Feature columns: 943\n",
      "03:53:44 | INFO  | Train: 7,473 rows (bad rate: 17.48%)\n",
      "03:53:44 | INFO  | Test: 1,869 rows (bad rate: 17.50%)\n",
      "03:53:44 | INFO  | OOT 2024Q3: 916 rows (bad rate: 17.69%)\n",
      "03:53:44 | INFO  | OOT 2024Q4: 926 rows (bad rate: 16.31%)\n",
      "03:53:44 | INFO  | OOT 2025Q1: 345 rows (bad rate: 20.29%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 943\n",
      "Train: 7,473 rows, bad rate: 17.48%\n",
      "Test:  1,869 rows, bad rate: 17.50%\n",
      "OOT 2024Q3: 916 rows, bad rate: 17.69%\n",
      "OOT 2024Q4: 926 rows, bad rate: 16.31%\n",
      "OOT 2025Q1: 345 rows, bad rate: 20.29%\n"
     ]
    }
   ],
   "source": [
    "datasets = load_and_split(\n",
    "    input_path=config.data.input_path,\n",
    "    train_end_date=config.splitting.train_end_date,\n",
    "    target_column=config.data.target_column,\n",
    "    date_column=config.data.date_column,\n",
    "    test_size=config.splitting.test_size,\n",
    "    stratify=config.splitting.stratify,\n",
    ")\n",
    "\n",
    "target = config.data.target_column\n",
    "features = list(datasets.feature_columns)\n",
    "X_train = datasets.train[features]\n",
    "y_train = datasets.train[target]\n",
    "X_test = datasets.test[features]\n",
    "y_test = datasets.test[target]\n",
    "\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Train: {len(datasets.train):,} rows, bad rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test:  {len(datasets.test):,} rows, bad rate: {y_test.mean():.2%}\")\n",
    "for label in datasets.oot_labels:\n",
    "    qdf = datasets.oot_quarters[label]\n",
    "    print(f\"OOT {label}: {len(qdf):,} rows, bad rate: {qdf[target].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Automated pre-pipeline checks: target validation, date validation, duplicates, leakage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:45 | INFO  | DATA_CHECK | PASS | Non-empty dataset | Dataset has 11,529 rows.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Target column exists | Target column 'target' present.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Target is binary | Target is binary (0/1).\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Target has no nulls | No null values in target.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Bad rate within range | Bad rate 17.4863% is within acceptable range.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Date column exists | Date column 'application_date' present and parseable.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Date range coverage | Date range covers 13 quarters (2022-01-01 to 2025-01-31).\n",
      "03:53:45 | WARNING | DATA_CHECK | WARNING | Features are numeric | 1 non-numeric feature(s) found.\n",
      "03:53:45 | WARNING | DATA_CHECK | WARNING | No duplicate IDs | 1,529 duplicate application_id values found.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Sufficient sample size | 11,529 rows available.\n",
      "03:53:45 | INFO  | DATA_CHECK | PASS | Leakage detection | No features with suspiciously high AUC detected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Report: 9 PASS, 2 WARNING, 0 FAIL\n",
      "  [+] Non-empty dataset: Dataset has 11,529 rows.\n",
      "  [+] Target column exists: Target column 'target' present.\n",
      "  [+] Target is binary: Target is binary (0/1).\n",
      "  [+] Target has no nulls: No null values in target.\n",
      "  [+] Bad rate within range: Bad rate 17.4863% is within acceptable range.\n",
      "  [+] Date column exists: Date column 'application_date' present and parseable.\n",
      "  [+] Date range coverage: Date range covers 13 quarters (2022-01-01 to 2025-01-31).\n",
      "  [!] Features are numeric: 1 non-numeric feature(s) found.\n",
      "  [!] No duplicate IDs: 1,529 duplicate application_id values found.\n",
      "  [+] Sufficient sample size: 11,529 rows available.\n",
      "  [+] Leakage detection: No features with suspiciously high AUC detected.\n"
     ]
    }
   ],
   "source": [
    "# Load raw data for data validator (it expects the full DataFrame)\n",
    "if config.data.input_path.endswith('.csv'):\n",
    "    df_raw = pd.read_csv(config.data.input_path)\n",
    "else:\n",
    "    df_raw = pd.read_parquet(config.data.input_path)\n",
    "\n",
    "data_validator = DataValidator(config)\n",
    "data_report = data_validator.validate(df_raw)\n",
    "print(data_report.summary())\n",
    "\n",
    "if data_report.has_critical_failures:\n",
    "    print(\"\\nCRITICAL FAILURES — review before proceeding:\")\n",
    "    for check in data_report.checks:\n",
    "        if check.status.value == \"FAIL\":\n",
    "            print(f\"  FAIL: {check.check_name} — {check.message}\")\n",
    "            print(f\"        Fix: {check.recommendation}\")\n",
    "\n",
    "del df_raw  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Steps\n",
    "\n",
    "Each step narrows the feature set. Run cells in order.\n",
    "\n",
    "Elimination results are collected for the Excel report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Constant Elimination\n",
    "Remove features with fewer than 2 distinct values (zero variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:45 | INFO  | CONSTANT | Eliminated 131 features (812 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Constant: 943 -> 812 features (131 eliminated)\n",
      "\n",
      "Eliminated features (131):\n",
      "                                       Feature  Unique_Count\n",
      "                     co_applicant_default_rate             1\n",
      "                            moved_to_unsecured             1\n",
      " installment_sale_defaulted_amount_std_last_6m             1\n",
      "             mortgage_recovered_count_last_24m             1\n",
      "      mortgage_recovered_total_amount_last_24m             1\n",
      "    mortgage_recovered_average_amount_last_24m             1\n",
      "installment_loan_recovered_amount_std_last_12m             1\n",
      "        mortgage_recovered_max_amount_last_24m             1\n",
      "         mortgage_recovered_amount_std_last_3m             1\n",
      "         mortgage_recovered_min_amount_last_3m             1\n"
     ]
    }
   ],
   "source": [
    "elimination_results = []\n",
    "\n",
    "const_elim = ConstantEliminator()\n",
    "const_result = const_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(const_result)\n",
    "features = const_result.kept_features\n",
    "\n",
    "print(f\"{const_result.step_name}: {const_result.n_kept + const_result.n_eliminated} -> \"\n",
    "      f\"{const_result.n_kept} features ({const_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if const_result.eliminated_features:\n",
    "    elim_df = const_result.details_df[const_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nEliminated features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Unique_Count\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Missing Elimination\n",
    "Remove features with missing rate above threshold on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:45 | INFO  | MISSING | Eliminated 0 features (812 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_Missing: 812 -> 812 features (0 eliminated)\n"
     ]
    }
   ],
   "source": [
    "missing_elim = MissingEliminator(max_missing_rate=config.steps.missing.threshold)\n",
    "missing_result = missing_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(missing_result)\n",
    "features = missing_result.kept_features\n",
    "\n",
    "print(f\"{missing_result.step_name}: {missing_result.n_kept + missing_result.n_eliminated} -> \"\n",
    "      f\"{missing_result.n_kept} features ({missing_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if missing_result.eliminated_features:\n",
    "    elim_df = missing_result.details_df[missing_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nEliminated features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Missing_Rate\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: IV Elimination\n",
    "Remove features with IV below `min_iv` (useless) or above `max_iv` (suspicious leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:46 | INFO  | IV | Eliminated 657 features (155 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_IV_Analysis: 812 -> 155 features (657 eliminated)\n",
      "\n",
      "IV Category Distribution:\n",
      "IV_Category\n",
      "useless       623\n",
      "weak           90\n",
      "medium         63\n",
      "suspicious     34\n",
      "strong          2\n",
      "\n",
      "Top 10 features by IV:\n",
      "                        Feature  IV_Score IV_Category  Univariate_AUC\n",
      "       is_monthly_payment_total    0.3401      strong          0.6298\n",
      "             default_count_ever    0.3026      strong          0.8466\n",
      "  installment_sale_total_amount    0.2941      medium          0.6280\n",
      "installment_sale_average_amount    0.2933      medium          0.6272\n",
      "                  is_avg_amount    0.2933      medium          0.6272\n",
      "    installment_sale_max_amount    0.2929      medium          0.6279\n",
      "    installment_sale_min_amount    0.2703      medium          0.6264\n",
      "      payment_to_exposure_ratio    0.2222      medium          0.6350\n",
      "             total_credit_count    0.2175      medium          0.6247\n",
      "    multi_product_default_count    0.2159      medium          0.8458\n"
     ]
    }
   ],
   "source": [
    "iv_elim = IVEliminator(min_iv=config.steps.iv.min_iv, max_iv=config.steps.iv.max_iv)\n",
    "iv_result = iv_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(iv_result)\n",
    "features = iv_result.kept_features\n",
    "\n",
    "# Extract IV scores for downstream use (correlation, selection, VIF)\n",
    "iv_scores = {}\n",
    "for _, row in iv_result.details_df.iterrows():\n",
    "    if row.get(\"IV_Score\") is not None:\n",
    "        iv_scores[row[\"Feature\"]] = row[\"IV_Score\"]\n",
    "\n",
    "print(f\"{iv_result.step_name}: {iv_result.n_kept + iv_result.n_eliminated} -> \"\n",
    "      f\"{iv_result.n_kept} features ({iv_result.n_eliminated} eliminated)\")\n",
    "\n",
    "# IV distribution\n",
    "print(f\"\\nIV Category Distribution:\")\n",
    "print(iv_result.details_df[\"IV_Category\"].value_counts().to_string())\n",
    "\n",
    "# Top features by IV\n",
    "kept_df = iv_result.details_df[iv_result.details_df[\"Status\"] == \"Kept\"].sort_values(\"IV_Score\", ascending=False)\n",
    "print(f\"\\nTop 10 features by IV:\")\n",
    "print(kept_df[[\"Feature\", \"IV_Score\", \"IV_Category\", \"Univariate_AUC\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: PSI Stability Elimination\n",
    "Remove features with unstable distributions within training data. PSI checks are built from config (quarterly, yearly, consecutive, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:46 | INFO  | PSI | Check QuarterlyPSICheck: 10 comparison(s)\n",
      "03:53:46 | INFO  | PSI | Check YearlyPSICheck: 3 comparison(s)\n",
      "03:53:46 | INFO  | PSI | Check ConsecutiveQuarterPSICheck: 9 comparison(s)\n",
      "03:53:46 | INFO  | PSI | Checking 155 features across 22 comparisons\n",
      "03:53:52 | INFO  | PSI | Eliminated 0 features (155 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_PSI_Stability: 155 -> 155 features (0 eliminated)\n"
     ]
    }
   ],
   "source": [
    "# Build PSI checks from config (same as run_model_development.py)\n",
    "check_map = {\n",
    "    \"quarterly\": QuarterlyPSICheck,\n",
    "    \"yearly\": YearlyPSICheck,\n",
    "    \"consecutive\": ConsecutiveQuarterPSICheck,\n",
    "    \"halfsplit\": HalfSplitPSICheck,\n",
    "}\n",
    "psi_checks = []\n",
    "for c in config.steps.psi.checks:\n",
    "    if c.type in check_map:\n",
    "        psi_checks.append(check_map[c.type]())\n",
    "    elif c.type == \"date_split\" and c.date:\n",
    "        psi_checks.append(DateSplitPSICheck(c.date, label=c.label))\n",
    "\n",
    "psi_elim = PSIEliminator(\n",
    "    critical_threshold=config.steps.psi.threshold,\n",
    "    checks=psi_checks,\n",
    ")\n",
    "psi_result = psi_elim.eliminate(\n",
    "    X_train, y_train, features,\n",
    "    train_dates=datasets.train[config.data.date_column],\n",
    ")\n",
    "elimination_results.append(psi_result)\n",
    "features = psi_result.kept_features\n",
    "\n",
    "print(f\"{psi_result.step_name}: {psi_result.n_kept + psi_result.n_eliminated} -> \"\n",
    "      f\"{psi_result.n_kept} features ({psi_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if psi_result.eliminated_features:\n",
    "    elim_df = psi_result.details_df[psi_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nUnstable features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Max_PSI\", \"Mean_PSI\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Correlation Elimination\n",
    "Greedy removal: among correlated pairs, keep the higher-IV feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:52 | INFO  | CORRELATION | Computing pearson correlation matrix for 155 features\n",
      "03:53:52 | INFO  | CORRELATION | Eliminated 74 features (81 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05_Correlation: 155 -> 81 features (74 eliminated)\n",
      "\n",
      "Correlated pairs (179):\n",
      "                      Feature_A                       Feature_B  Correlation                                                                    Decision\n",
      "             default_count_ever     multi_product_default_count       0.9762                multi_product_default_count eliminated by default_count_ever\n",
      "  installment_sale_total_amount installment_sale_average_amount       0.9570 installment_sale_average_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount                   is_avg_amount       0.9570                   is_avg_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount     installment_sale_max_amount       0.9798     installment_sale_max_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount     installment_sale_min_amount       0.9109     installment_sale_min_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount          installment_sale_count       0.8172          installment_sale_count eliminated by installment_sale_total_amount\n",
      "installment_sale_average_amount                   is_avg_amount       1.0000                                                             both eliminated\n",
      "installment_sale_average_amount     installment_sale_max_amount       0.9890                                                             both eliminated\n",
      "installment_sale_average_amount     installment_sale_min_amount       0.9880                                                             both eliminated\n",
      "                  is_avg_amount     installment_sale_max_amount       0.9890                                                             both eliminated\n"
     ]
    }
   ],
   "source": [
    "corr_elim = CorrelationEliminator(max_correlation=config.steps.correlation.threshold)\n",
    "corr_result = corr_elim.eliminate(X_train, y_train, features, iv_scores=iv_scores)\n",
    "elimination_results.append(corr_result)\n",
    "features = corr_result.kept_features\n",
    "\n",
    "# Save correlation pairs for the Excel report\n",
    "corr_pairs_df = getattr(corr_elim, \"corr_pairs_df\", None)\n",
    "\n",
    "print(f\"{corr_result.step_name}: {corr_result.n_kept + corr_result.n_eliminated} -> \"\n",
    "      f\"{corr_result.n_kept} features ({corr_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if corr_pairs_df is not None and len(corr_pairs_df) > 0:\n",
    "    print(f\"\\nCorrelated pairs ({len(corr_pairs_df)}):\")\n",
    "    print(corr_pairs_df[[\"Feature_A\", \"Feature_B\", \"Correlation\", \"Decision\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sequential Feature Selection\n",
    "CV-based forward (or backward) selection with elbow detection (1-SE rule).\n",
    "Saves a performance chart to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:53:52 | INFO  | Output directory: outputs/model_development/20260212_035352_0b66a1\n",
      "03:53:52 | INFO  | Config saved to outputs/model_development/20260212_035352_0b66a1/config/pipeline_config.yaml\n",
      "03:53:52 | INFO  | SELECTION | Starting forward selection with 81 candidates, cv=5, max_features=3, tolerance=0.001, patience=3\n",
      "03:53:52 | INFO  | SELECTION | Forward step 1/3: evaluating 81 candidates...\n",
      "03:54:28 | INFO  | SELECTION | Step 1: ADDED default_count_ever (IV=0.3026), CV AUC=0.8466 ± 0.0131\n",
      "03:54:28 | INFO  | SELECTION | Forward step 2/3: evaluating 80 candidates...\n",
      "03:55:12 | INFO  | SELECTION | Step 2: ADDED recovered_total_amount (IV=0.1737), CV AUC=0.8626 ± 0.0120\n",
      "03:55:12 | INFO  | SELECTION | Forward step 3/3: evaluating 79 candidates...\n",
      "03:56:10 | INFO  | SELECTION | Step 3: ADDED newest_credit_age_months (IV=0.0238), CV AUC=0.8730 ± 0.0056\n",
      "03:56:10 | INFO  | SELECTION | Optimal feature count: 3 (1-SE rule)\n",
      "03:56:10 | INFO  | SELECTION | Selected 3 features: ['default_count_ever', 'recovered_total_amount', 'newest_credit_age_months']\n",
      "03:56:10 | INFO  | SELECTION | Performance chart saved to outputs/model_development/20260212_035352_0b66a1/reports/selection_chart_20260212_035610.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (3):\n",
      "  1. default_count_ever (IV=0.3026)\n",
      "  2. recovered_total_amount (IV=0.1737)\n",
      "  3. newest_credit_age_months (IV=0.0238)\n",
      "\n",
      "Selection chart: outputs/model_development/20260212_035352_0b66a1/reports/selection_chart_20260212_035610.png\n",
      "\n",
      "Selection steps:\n",
      " Step  N_Features  Mean_CV_AUC  Std_CV_AUC  Is_Optimal\n",
      "    1           1     0.846575    0.013117       False\n",
      "    2           2     0.862620    0.011953       False\n",
      "    3           3     0.872956    0.005596        True\n"
     ]
    }
   ],
   "source": [
    "# Create output manager for saving outputs\n",
    "output_manager = OutputManager(config)\n",
    "\n",
    "# Save config snapshot\n",
    "if config.reproducibility.save_config:\n",
    "    save_config(config, str(output_manager.run_dir / \"config\" / \"pipeline_config.yaml\"))\n",
    "\n",
    "# Selection chart goes into the run's reports directory\n",
    "selection_output_dir = str(output_manager.run_dir / \"reports\")\n",
    "\n",
    "selected_features, selection_df, chart_path = sequential_feature_selection(\n",
    "    X_train=X_train[features],\n",
    "    y_train=y_train,\n",
    "    X_test=X_test[features],\n",
    "    y_test=y_test,\n",
    "    features=features,\n",
    "    direction=config.steps.selection.method,\n",
    "    cv=config.steps.selection.cv,\n",
    "    min_features=config.steps.selection.min_features,\n",
    "    max_features=3,# config.steps.selection.max_features,\n",
    "    tolerance=config.steps.selection.tolerance,\n",
    "    patience=config.steps.selection.patience,\n",
    "    iv_scores=iv_scores,\n",
    "    xgb_params=config.model.params,\n",
    "    output_dir=selection_output_dir,\n",
    ")\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    iv = iv_scores.get(feat, 0)\n",
    "    print(f\"  {i}. {feat} (IV={iv:.4f})\")\n",
    "\n",
    "print(f\"\\nSelection chart: {chart_path}\")\n",
    "\n",
    "# Show selection steps\n",
    "added = selection_df[selection_df.get(\"Is_Optimal\", False) | True]  # show all steps\n",
    "print(f\"\\nSelection steps:\")\n",
    "cols = [c for c in [\"Step\", \"N_Features\", \"Mean_CV_AUC\", \"Std_CV_AUC\", \"Is_Optimal\"] if c in selection_df.columns]\n",
    "print(selection_df[cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: VIF Multicollinearity Check\n",
    "Post-selection VIF check. Iteratively removes features with VIF above threshold, preserving higher-IV features when `iv_aware=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:56:10 | INFO  | VIF | Starting VIF check with 3 features, threshold=5.0\n",
      "03:56:10 | INFO  | VIF | Iteration 1: max VIF=1.1548, all features below threshold\n",
      "03:56:10 | INFO  | VIF | Eliminated 0 features (3 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07_VIF: 3 -> 3 features (0 eliminated)\n",
      "All features passed VIF check.\n",
      "                 Feature  VIF_Initial  VIF_Final  IV_Score\n",
      "      default_count_ever       1.1548     1.1548    0.3026\n",
      "  recovered_total_amount       1.1508     1.1508    0.1737\n",
      "newest_credit_age_months       1.0046     1.0046    0.0238\n",
      "\n",
      "Final selected features (3): ['default_count_ever', 'recovered_total_amount', 'newest_credit_age_months']\n"
     ]
    }
   ],
   "source": [
    "vif_result = None\n",
    "if config.steps.vif.enabled and len(selected_features) > 2:\n",
    "    vif_elim = VIFEliminator(\n",
    "        threshold=config.steps.vif.threshold,\n",
    "        iv_aware=config.steps.vif.iv_aware,\n",
    "    )\n",
    "    vif_result = vif_elim.eliminate(\n",
    "        X_train, y_train, selected_features, iv_scores=iv_scores,\n",
    "    )\n",
    "    elimination_results.append(vif_result)\n",
    "    selected_features = vif_result.kept_features\n",
    "\n",
    "    print(f\"{vif_result.step_name}: {vif_result.n_kept + vif_result.n_eliminated} -> \"\n",
    "          f\"{vif_result.n_kept} features ({vif_result.n_eliminated} eliminated)\")\n",
    "\n",
    "    if vif_result.n_eliminated > 0:\n",
    "        elim_df = vif_result.details_df[vif_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "        print(f\"\\nEliminated by VIF:\")\n",
    "        print(elim_df[[\"Feature\", \"VIF_Initial\", \"IV_Score\", \"Elimination_Round\"]].to_string(index=False))\n",
    "    else:\n",
    "        print(\"All features passed VIF check.\")\n",
    "        print(vif_result.details_df[[\"Feature\", \"VIF_Initial\", \"VIF_Final\", \"IV_Score\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"VIF check skipped (disabled or too few features)\")\n",
    "\n",
    "print(f\"\\nFinal selected features ({len(selected_features)}): {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Hyperparameter Tuning (Optuna)\n",
    "Optuna-based Bayesian optimization with TPE sampler and stratified CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tuning_df = None\nbest_params = None\n\nif config.model.tuning.enabled:\n    print(f\"Running Optuna tuning: {config.model.tuning.n_trials} trials, \"\n          f\"stability_weight={config.model.tuning.stability_weight}, n_jobs={n_jobs}\")\n    best_params, tuning_df, final_model = tune_hyperparameters(\n        X_train=X_train,\n        y_train=y_train,\n        X_test=X_test,\n        y_test=y_test,\n        features=selected_features,\n        n_trials=config.model.tuning.n_trials,\n        timeout=config.model.tuning.timeout,\n        cv=config.model.tuning.cv,\n        n_jobs=n_jobs,\n        oot_quarters=datasets.oot_quarters,\n        target_column=config.data.target_column,\n        stability_weight=config.model.tuning.stability_weight,\n    )\n    print(f\"\\nBest params: {best_params}\")\n    if tuning_df is not None:\n        print(f\"\\nTrial history ({len(tuning_df)} trials):\")\n        print(tuning_df.head(10).to_string(index=False))\nelse:\n    print(\"Tuning disabled — training with default/configured params\")\n    # Train with configured params (same as pipeline._train_default_model)\n    params = (config.model.params or {}).copy()\n    if not params:\n        params = {\n            \"objective\": \"binary:logistic\",\n            \"eval_metric\": \"auc\",\n            \"max_depth\": 6,\n            \"learning_rate\": 0.1,\n            \"n_estimators\": 300,\n            \"subsample\": 0.8,\n            \"colsample_bytree\": 0.8,\n            \"random_state\": 42,\n            \"n_jobs\": -1,\n            \"verbosity\": 0,\n        }\n    # Auto-balance\n    neg_count = (y_train == 0).sum()\n    pos_count = (y_train == 1).sum()\n    if params.pop(\"scale_pos_weight\", None) == \"auto\":\n        params[\"scale_pos_weight\"] = neg_count / pos_count\n    # early_stopping_rounds to constructor (xgboost >= 2.0)\n    early_stopping_rounds = params.pop(\"early_stopping_rounds\", 30)\n    params[\"early_stopping_rounds\"] = early_stopping_rounds\n\n    final_model = xgb.XGBClassifier(**params)\n    final_model.fit(\n        X_train[selected_features], y_train,\n        eval_set=[(X_test[selected_features], y_test)],\n        verbose=False,\n    )\n    print(f\"Model trained with {final_model.n_estimators} estimators\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Model Evaluation\n",
    "Evaluate the final model on Train, Test, and each OOT quarter. Produces performance table, lift tables, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:27 | INFO  | OOT | Train: AUC=0.9004, Gini=0.8007, KS=0.7012\n",
      "03:57:27 | INFO  | OOT | Test: AUC=0.8634, Gini=0.7267, KS=0.6842\n",
      "03:57:27 | INFO  | OOT | OOT_2024Q3: AUC=0.8779, Gini=0.7557, KS=0.7228\n",
      "03:57:27 | INFO  | OOT | OOT_2024Q4: AUC=0.8547, Gini=0.7095, KS=0.6590\n",
      "03:57:27 | INFO  | OOT | OOT_2025Q1: AUC=0.8596, Gini=0.7192, KS=0.7452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by period:\n",
      "    Period  N_Samples  N_Bads  Bad_Rate    AUC   Gini     KS  Precision_at_10pct  Lift_at_10pct\n",
      "     Train       7473    1306    0.1748 0.9004 0.8007 0.7012              0.7751           4.44\n",
      "      Test       1869     327    0.1750 0.8634 0.7267 0.6842              0.6613           3.78\n",
      "OOT_2024Q3        916     162    0.1769 0.8779 0.7557 0.7228              0.7692           4.35\n",
      "OOT_2024Q4        926     151    0.1631 0.8547 0.7095 0.6590              0.7391           4.53\n",
      "OOT_2025Q1        345      70    0.2029 0.8596 0.7192 0.7452              0.8235           4.06\n",
      "\n",
      "Feature importance:\n",
      "                 Feature  Importance  Rank  Cumulative_Importance\n",
      "      default_count_ever    0.982062     1               0.982062\n",
      "  recovered_total_amount    0.012428     2               0.994490\n",
      "newest_credit_age_months    0.005510     3               1.000000\n"
     ]
    }
   ],
   "source": [
    "performance_df, lift_tables, importance_df = evaluate_model_quarterly(\n",
    "    model=final_model,\n",
    "    selected_features=selected_features,\n",
    "    train_df=datasets.train,\n",
    "    test_df=datasets.test,\n",
    "    oot_quarters=datasets.oot_quarters,\n",
    "    target_column=config.data.target_column,\n",
    "    importance_type=config.evaluation.importance_type,\n",
    ")\n",
    "\n",
    "print(\"Performance by period:\")\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nFeature importance:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Tables\n",
    "Decile-based lift tables for each evaluation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Lift Table: Train\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7079     0.7724      0.7150    747   579    0.7751 4.435163        747       579      0.775100  4.435165      0.443338\n",
      "     2     0.6267     0.7078      0.6814    748   422    0.5642 3.228382       1495      1001      0.669565  3.831287      0.766462\n",
      "     3     0.3492     0.6263      0.4780    742   112    0.1509 0.863458       2237      1113      0.497541  2.846958      0.852221\n",
      "     4     0.3398     0.3492      0.3437    745    52    0.0698 0.399399       2982      1165      0.390677  2.235476      0.892037\n",
      "     5     0.3325     0.3389      0.3342    586    26    0.0444 0.254059       3568      1191      0.333800  1.910024      0.911945\n",
      "     6     0.3297     0.3315      0.3314    735    38    0.0517 0.295830       4303      1229      0.285615  1.634302      0.941041\n",
      "     7     0.3224     0.3296      0.3287    764    31    0.0406 0.232315       5067      1260      0.248668  1.422890      0.964778\n",
      "     8     0.3177     0.3217      0.3207    859    19    0.0221 0.126457       5926      1279      0.215829  1.234982      0.979326\n",
      "     9     0.3091     0.3175      0.3150    770    24    0.0312 0.178528       6696      1303      0.194594  1.113476      0.997703\n",
      "    10     0.2659     0.3082      0.2946    777     3    0.0039 0.022316       7473      1306      0.174762  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: Test\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7089     0.7724      0.7172    156   105    0.6731 3.847168        156       105      0.673077  3.847036      0.321101\n",
      "     2     0.6486     0.7085      0.6948    218   127    0.5826 3.329906        374       232      0.620321  3.545504      0.709480\n",
      "     3     0.3606     0.6481      0.5496    181    43    0.2376 1.358026        555       275      0.495495  2.832052      0.840979\n",
      "     4     0.3454     0.3560      0.3511     86     2    0.0233 0.133173        641       277      0.432137  2.469922      0.847095\n",
      "     5     0.3325     0.3436      0.3390    270    13    0.0481 0.274920        911       290      0.318332  1.819454      0.886850\n",
      "     6     0.3297     0.3315      0.3314    156     7    0.0449 0.256630       1067       297      0.278351  1.590939      0.908257\n",
      "     7     0.3224     0.3296      0.3285    193     7    0.0363 0.207476       1260       304      0.241270  1.379001      0.929664\n",
      "     8     0.3177     0.3217      0.3207    206     8    0.0388 0.221765       1466       312      0.212824  1.216416      0.954128\n",
      "     9     0.3091     0.3175      0.3144    206     8    0.0388 0.221765       1672       320      0.191388  1.093894      0.978593\n",
      "    10     0.2659     0.3082      0.2959    197     7    0.0355 0.202904       1869       327      0.174960  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2024Q3\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7040     0.7637      0.7148     89    68    0.7640 4.319901         89        68      0.764045  4.320155      0.419753\n",
      "     2     0.6225     0.7037      0.6749     94    58    0.6170 3.488716        183       126      0.688525  3.893139      0.777778\n",
      "     3     0.3528     0.6211      0.5119     91    12    0.1319 0.745805        274       138      0.503650  2.847797      0.851852\n",
      "     4     0.3436     0.3496      0.3450     91     2    0.0220 0.124395        365       140      0.383562  2.168781      0.864198\n",
      "     5     0.3325     0.3429      0.3356     89     3    0.0337 0.190551        454       143      0.314978  1.780987      0.882716\n",
      "     6     0.3297     0.3315      0.3314     75     2    0.0267 0.150970        529       145      0.274102  1.549861      0.895062\n",
      "     7     0.3224     0.3296      0.3284     88     5    0.0568 0.321165        617       150      0.243112  1.374632      0.925926\n",
      "     8     0.3177     0.3217      0.3206    100     4    0.0400 0.226173        717       154      0.214784  1.214457      0.950617\n",
      "     9     0.3091     0.3175      0.3149    104     4    0.0385 0.217691        821       158      0.192448  1.088164      0.975309\n",
      "    10     0.2745     0.3082      0.2953     95     4    0.0421 0.238047        916       162      0.176856  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2024Q4\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7032     0.7484      0.7113     89    65    0.7303 4.478528         89        65      0.730337  4.478756      0.430464\n",
      "     2     0.6126     0.7029      0.6637     96    42    0.4375 2.682947        185       107      0.578378  3.546877      0.708609\n",
      "     3     0.3454     0.6121      0.4411     93    13    0.1398 0.857317        278       120      0.431655  2.647101      0.794702\n",
      "     4     0.3375     0.3436      0.3428     88     5    0.0568 0.348323        366       125      0.341530  2.094416      0.827815\n",
      "     5     0.3325     0.3357      0.3339     66     4    0.0606 0.371626        432       129      0.298611  1.831218      0.854305\n",
      "     6     0.3297     0.3315      0.3314     98     6    0.0612 0.375306        530       135      0.254717  1.562039      0.894040\n",
      "     7     0.3224     0.3296      0.3289     91     2    0.0220 0.134914        621       137      0.220612  1.352892      0.907285\n",
      "     8     0.3177     0.3217      0.3206    117     7    0.0598 0.366721        738       144      0.195122  1.196576      0.953642\n",
      "     9     0.3091     0.3175      0.3150     92     4    0.0435 0.266762        830       148      0.178313  1.093497      0.980132\n",
      "    10     0.2659     0.3082      0.2943     96     3    0.0312 0.191332        926       151      0.163067  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2025Q1\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7089     0.7384      0.7137     34    28    0.8235 4.058679         34        28      0.823529  4.058824      0.400000\n",
      "     2     0.6259     0.7085      0.6845     34    25    0.7353 3.623979         68        53      0.779412  3.841387      0.757143\n",
      "     3     0.3478     0.6242      0.4365     30     5    0.1667 0.821593         98        58      0.591837  2.916910      0.828571\n",
      "     4     0.3349     0.3436      0.3425     40     1    0.0250 0.123214        138        59      0.427536  2.107143      0.842857\n",
      "     5     0.3330     0.3347      0.3339     16     0    0.0000 0.000000        154        59      0.383117  1.888219      0.842857\n",
      "     6     0.3297     0.3315      0.3313     26     0    0.0000 0.000000        180        59      0.327778  1.615476      0.842857\n",
      "     7     0.3202     0.3296      0.3257     61     4    0.0656 0.323314        241        63      0.261411  1.288382      0.900000\n",
      "     8     0.3147     0.3194      0.3176     35     1    0.0286 0.140957        276        64      0.231884  1.142857      0.914286\n",
      "     9     0.3082     0.3145      0.3122     32     3    0.0938 0.462300        308        67      0.217532  1.072124      0.957143\n",
      "    10     0.2745     0.3077      0.2928     37     3    0.0811 0.399707        345        70      0.202899  1.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "for period, lt in lift_tables.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Lift Table: {period}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(lt.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Evaluation Enhanced Steps\n",
    "\n",
    "Score PSI, Bootstrap CI, Calibration, SHAP, and Validation — same as the pipeline script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score PSI (train vs OOT periods):\n",
      "Period_1   Period_2    PSI Status\n",
      "   Train OOT_2024Q3 0.0105 Stable\n",
      "   Train OOT_2024Q4 0.0162 Stable\n",
      "   Train OOT_2025Q1 0.0443 Stable\n"
     ]
    }
   ],
   "source": [
    "# Step 9a: Score PSI — stability of predicted scores between train and OOT\n",
    "score_psi_df = None\n",
    "if config.evaluation.calculate_score_psi:\n",
    "    train_probs = final_model.predict_proba(datasets.train[selected_features])[:, 1]\n",
    "    oot_scores = {}\n",
    "    for label in sorted(datasets.oot_quarters.keys()):\n",
    "        qdf = datasets.oot_quarters[label]\n",
    "        oot_scores[f\"OOT_{label}\"] = final_model.predict_proba(qdf[selected_features])[:, 1]\n",
    "    score_psi_df = compute_score_psi(train_probs, oot_scores)\n",
    "    print(\"Score PSI (train vs OOT periods):\")\n",
    "    print(score_psi_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Score PSI disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap AUC Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:28 | INFO  | Bootstrap | Train: AUC=0.9004 [0.8903, 0.9107] (1000 iterations)\n",
      "03:57:28 | INFO  | Bootstrap | Test: AUC=0.8634 [0.8369, 0.8873] (1000 iterations)\n",
      "03:57:29 | INFO  | Bootstrap | OOT_2024Q3: AUC=0.8779 [0.8403, 0.9115] (1000 iterations)\n",
      "03:57:29 | INFO  | Bootstrap | OOT_2024Q4: AUC=0.8547 [0.8130, 0.8945] (1000 iterations)\n",
      "03:57:29 | INFO  | Bootstrap | OOT_2025Q1: AUC=0.8596 [0.7961, 0.9192] (1000 iterations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap AUC Confidence Intervals:\n",
      "    Period    AUC  CI_Lower  CI_Upper  N_Bootstrap\n",
      "     Train 0.9004    0.8903    0.9107         1000\n",
      "      Test 0.8634    0.8369    0.8873         1000\n",
      "OOT_2024Q3 0.8779    0.8403    0.9115         1000\n",
      "OOT_2024Q4 0.8547    0.8130    0.8945         1000\n",
      "OOT_2025Q1 0.8596    0.7961    0.9192         1000\n"
     ]
    }
   ],
   "source": [
    "# Step 9b: Bootstrap CI — confidence intervals on AUC for each period\n",
    "bootstrap_df = None\n",
    "if config.evaluation.bootstrap.enabled:\n",
    "    periods_for_bootstrap = [(\"Train\", datasets.train), (\"Test\", datasets.test)]\n",
    "    for label in sorted(datasets.oot_quarters.keys()):\n",
    "        periods_for_bootstrap.append((f\"OOT_{label}\", datasets.oot_quarters[label]))\n",
    "\n",
    "    bootstrap_df = bootstrap_auc_ci(\n",
    "        model=final_model,\n",
    "        selected_features=selected_features,\n",
    "        datasets=periods_for_bootstrap,\n",
    "        target_column=config.data.target_column,\n",
    "        n_iterations=config.evaluation.bootstrap.n_iterations,\n",
    "        confidence_level=config.evaluation.bootstrap.confidence_level,\n",
    "    )\n",
    "\n",
    "    # Merge CI columns into performance_df\n",
    "    if bootstrap_df is not None and not bootstrap_df.empty:\n",
    "        ci_cols = bootstrap_df[[\"Period\", \"CI_Lower\", \"CI_Upper\"]].copy()\n",
    "        performance_df = performance_df.merge(ci_cols, on=\"Period\", how=\"left\")\n",
    "\n",
    "    print(\"Bootstrap AUC Confidence Intervals:\")\n",
    "    print(bootstrap_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Bootstrap CI disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration (platt):\n",
      "  Brier score: 0.1577 -> 0.0910\n",
      "  ECE:         0.2471 -> 0.0290\n",
      "  H-L chi2:    16.7060 (p=0.0333)\n"
     ]
    }
   ],
   "source": [
    "# Step 9c: Calibration — fit on test set, report Brier/ECE improvement\n",
    "calibration_dict = None\n",
    "if config.evaluation.calibration.enabled:\n",
    "    from src.evaluation.calibrator import ModelCalibrator\n",
    "    calibrator = ModelCalibrator(method=config.evaluation.calibration.method)\n",
    "    test_probs = final_model.predict_proba(datasets.test[selected_features])[:, 1]\n",
    "    y_test_vals = datasets.test[config.data.target_column].values\n",
    "    calibrator.fit(y_test_vals, test_probs)\n",
    "    cal_result = calibrator.get_calibration_result(y_test_vals, test_probs)\n",
    "    calibration_dict = cal_result.to_dict()\n",
    "\n",
    "    print(f\"Calibration ({cal_result.method}):\")\n",
    "    print(f\"  Brier score: {cal_result.brier_score_before:.4f} -> {cal_result.brier_score_after:.4f}\")\n",
    "    print(f\"  ECE:         {cal_result.ece_before:.4f} -> {cal_result.ece_after:.4f}\")\n",
    "    print(f\"  H-L chi2:    {cal_result.hosmer_lemeshow_chi2:.4f} (p={cal_result.hosmer_lemeshow_pvalue:.4f})\")\n",
    "else:\n",
    "    print(\"Calibration disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:38 | INFO  | Sampled 500 rows from 7473 for SHAP computation.\n",
      "03:57:38 | INFO  | Computed SHAP values: 500 samples x 3 features.\n",
      "03:57:38 | INFO  | Saved SHAP bar plot: outputs/model_development/20260212_035352_0b66a1/reports/shap_summary_bar.png\n",
      "03:57:38 | INFO  | Saved SHAP beeswarm plot: outputs/model_development/20260212_035352_0b66a1/reports/shap_beeswarm.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP Summary:\n",
      "                 Feature  Mean_Abs_SHAP  Rank\n",
      "      default_count_ever       0.729693     1\n",
      "newest_credit_age_months       0.042810     2\n",
      "  recovered_total_amount       0.037806     3\n",
      "\n",
      "SHAP plots saved: ['outputs/model_development/20260212_035352_0b66a1/reports/shap_summary_bar.png', 'outputs/model_development/20260212_035352_0b66a1/reports/shap_beeswarm.png']\n"
     ]
    }
   ],
   "source": [
    "# Step 9d: SHAP — TreeExplainer values + summary plots\n",
    "shap_summary = None\n",
    "shap_plot_paths = None\n",
    "if config.evaluation.shap.enabled:\n",
    "    from src.model_development.shap_analyzer import (\n",
    "        compute_shap_values,\n",
    "        shap_summary_df,\n",
    "        save_shap_plots,\n",
    "    )\n",
    "    shap_vals, feat_names, X_shap = compute_shap_values(\n",
    "        model=final_model,\n",
    "        X=datasets.train[selected_features],\n",
    "        max_samples=config.evaluation.shap.max_samples,\n",
    "    )\n",
    "    shap_summary = shap_summary_df(shap_vals, feat_names)\n",
    "\n",
    "    # Save plots to run directory\n",
    "    shap_output_dir = str(output_manager.run_dir / \"reports\")\n",
    "    shap_plot_paths = save_shap_plots(shap_vals, X_shap, shap_output_dir)\n",
    "\n",
    "    print(\"SHAP Summary:\")\n",
    "    print(shap_summary.to_string(index=False))\n",
    "    if shap_plot_paths:\n",
    "        print(f\"\\nSHAP plots saved: {shap_plot_paths}\")\n",
    "else:\n",
    "    print(\"SHAP analysis disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Checks\n",
    "Automated quality checks: discrimination, overfitting, OOT stability, score PSI, concentration, monotonicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:46 | INFO  | MODEL_CHECK | PASS | Discrimination (AUC) | All periods have AUC >= 0.65 (min observed: 0.8547).\n",
      "03:57:46 | INFO  | MODEL_CHECK | PASS | Discrimination (Gini) | All Gini values >= 0.3000 (min: 0.7095).\n",
      "03:57:46 | INFO  | MODEL_CHECK | PASS | Overfitting | AUC gap Train-Test = 0.0370 within threshold 0.0500.\n",
      "03:57:46 | INFO  | MODEL_CHECK | PASS | OOT stability | All OOT periods within 0.0800 of test AUC (min OOT AUC: 0.8547).\n",
      "03:57:46 | INFO  | MODEL_CHECK | PASS | Score PSI | Score PSI 0.0443 within threshold 0.2500.\n",
      "03:57:46 | WARNING | MODEL_CHECK | WARNING | Feature concentration | Feature 'default_count_ever' contributes 98.21% of total importance (threshold: 50%).\n",
      "03:57:46 | INFO  | MODEL_CHECK | PASS | OOT sample size | All OOT periods have >= 30 bads (min: 70).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Report: 6 PASS, 1 WARNING, 0 FAIL\n",
      "  [+] Discrimination (AUC): All periods have AUC >= 0.65 (min observed: 0.8547).\n",
      "  [+] Discrimination (Gini): All Gini values >= 0.3000 (min: 0.7095).\n",
      "  [+] Overfitting: AUC gap Train-Test = 0.0370 within threshold 0.0500.\n",
      "  [+] OOT stability: All OOT periods within 0.0800 of test AUC (min OOT AUC: 0.8547).\n",
      "  [+] Score PSI: Score PSI 0.0443 within threshold 0.2500.\n",
      "  [!] Feature concentration: Feature 'default_count_ever' contributes 98.21% of total importance (threshold: 50%).\n",
      "  [+] OOT sample size: All OOT periods have >= 30 bads (min: 70).\n",
      "\n",
      "Recommendations:\n",
      "  [WARNING] Feature concentration: Consider removing dominant feature and retraining for a more robust model.\n"
     ]
    }
   ],
   "source": [
    "# Step 9e: Validation — same checks as the pipeline script\n",
    "validation_report_df = None\n",
    "has_critical_failures = False\n",
    "if config.validation.enabled:\n",
    "    model_validator = ModelValidator(config)\n",
    "    val_report = model_validator.validate(\n",
    "        performance_df=performance_df,\n",
    "        importance_df=importance_df,\n",
    "        score_psi_df=score_psi_df,\n",
    "    )\n",
    "    validation_report_df = val_report.to_dataframe()\n",
    "    has_critical_failures = val_report.has_critical_failures\n",
    "\n",
    "    print(val_report.summary())\n",
    "\n",
    "    # Recommendations for failures/warnings\n",
    "    issues = [c for c in val_report.checks if c.status.value != \"PASS\"]\n",
    "    if issues:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for check in issues:\n",
    "            if check.recommendation:\n",
    "                print(f\"  [{check.status.value}] {check.check_name}: {check.recommendation}\")\n",
    "else:\n",
    "    print(\"Validation checks disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Outputs & Generate Excel Report\n",
    "\n",
    "Produces the same Excel report with all sheets as the script pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:51 | INFO  | EXCEL | Embedded selection chart in 06_Selection\n",
      "03:57:51 | INFO  | EXCEL | Embedded SHAP plot in 10_SHAP\n",
      "03:57:51 | INFO  | COMPLETE | Excel saved: outputs/model_development/20260212_035352_0b66a1/reports/model_dev_20260212_035352_0b66a1.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: outputs/model_development/20260212_035352_0b66a1/data/model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:51 | INFO  | Run metadata saved to outputs/model_development/20260212_035352_0b66a1/run_metadata.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run ID: 20260212_035352_0b66a1\n",
      "Excel report: outputs/model_development/20260212_035352_0b66a1/reports/model_dev_20260212_035352_0b66a1.xlsx\n",
      "Selection chart: outputs/model_development/20260212_035352_0b66a1/reports/selection_chart_20260212_035610.png\n",
      "SHAP plots: ['outputs/model_development/20260212_035352_0b66a1/reports/shap_summary_bar.png', 'outputs/model_development/20260212_035352_0b66a1/reports/shap_beeswarm.png']\n",
      "All outputs: outputs/model_development/20260212_035352_0b66a1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Build summary dict (same structure as pipeline._build_summary)\n",
    "n_total = len(datasets.feature_columns)\n",
    "n_after_const = const_result.n_kept\n",
    "n_after_missing = missing_result.n_kept\n",
    "n_after_iv = iv_result.n_kept\n",
    "n_after_psi = psi_result.n_kept\n",
    "n_after_corr = corr_result.n_kept\n",
    "n_after_sel = len(selected_features) + (vif_result.n_eliminated if vif_result else 0)\n",
    "n_after_vif = len(selected_features)\n",
    "\n",
    "train_dates = datasets.train[config.data.date_column]\n",
    "oot_labels_str = \", \".join(datasets.oot_labels) if datasets.oot_labels else \"None\"\n",
    "\n",
    "summary = {\n",
    "    \"Run Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Run ID\": output_manager.run_id,\n",
    "    \"Input File\": config.data.input_path,\n",
    "    \"Train End Date\": config.splitting.train_end_date,\n",
    "    \"Train Period\": f\"{train_dates.min().strftime('%Y-%m-%d')} to {train_dates.max().strftime('%Y-%m-%d')}\",\n",
    "    \"OOT Periods\": oot_labels_str,\n",
    "    \"Train Rows\": len(datasets.train),\n",
    "    \"Test Rows\": len(datasets.test),\n",
    "    \"Train Bad Rate\": f\"{datasets.train[target].mean():.2%}\",\n",
    "    \"Test Bad Rate\": f\"{datasets.test[target].mean():.2%}\",\n",
    "    \"\": \"\",\n",
    "    \"Total Features\": n_total,\n",
    "    \"After Constant Elimination\": f\"{n_after_const} ({n_total - n_after_const} eliminated)\",\n",
    "    \"After Missing Elimination\": f\"{n_after_missing} ({n_after_const - n_after_missing} eliminated)\",\n",
    "    \"After IV Elimination\": f\"{n_after_iv} ({n_after_missing - n_after_iv} eliminated)\",\n",
    "    \"After PSI Elimination\": f\"{n_after_psi} ({n_after_iv - n_after_psi} eliminated)\",\n",
    "    \"After Correlation Elimination\": f\"{n_after_corr} ({n_after_psi - n_after_corr} eliminated)\",\n",
    "    \"After Sequential Selection\": f\"{n_after_sel} ({n_after_corr - n_after_sel} skipped)\",\n",
    "    \"After VIF Check\": f\"{n_after_vif} ({n_after_sel - n_after_vif} eliminated)\",\n",
    "    \" \": \"\",\n",
    "    \"Selection Method\": config.steps.selection.method,\n",
    "    \"Selection CV Folds\": config.steps.selection.cv,\n",
    "}\n",
    "\n",
    "if config.model.tuning.enabled and best_params:\n",
    "    summary[\"Tuning Enabled\"] = \"Yes\"\n",
    "    summary[\"Tuning Trials\"] = config.model.tuning.n_trials\n",
    "    best_auc = best_params.get(\"_best_cv_auc\", \"N/A\")\n",
    "    summary[\"Tuning Best CV AUC\"] = best_auc\n",
    "else:\n",
    "    summary[\"Tuning Enabled\"] = \"No\"\n",
    "\n",
    "summary[\"  \"] = \"\"\n",
    "for _, row in performance_df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    summary[f\"AUC {period}\"] = row[\"AUC\"]\n",
    "    summary[f\"Gini {period}\"] = row[\"Gini\"]\n",
    "\n",
    "summary[\"   \"] = \"\"\n",
    "summary[\"IV Range\"] = f\"[{config.steps.iv.min_iv}, {config.steps.iv.max_iv}]\"\n",
    "summary[\"Missing Threshold\"] = f\"{config.steps.missing.threshold:.0%}\"\n",
    "summary[\"PSI Threshold\"] = str(config.steps.psi.threshold)\n",
    "summary[\"Correlation Threshold\"] = str(config.steps.correlation.threshold)\n",
    "summary[\"VIF Threshold\"] = str(config.steps.vif.threshold) if config.steps.vif.enabled else \"Disabled\"\n",
    "\n",
    "# Generate Excel report (same call as pipeline.py)\n",
    "excel_path = str(output_manager.run_dir / \"reports\" / f\"model_dev_{output_manager.run_id}.xlsx\")\n",
    "\n",
    "excel_reporter.generate_report(\n",
    "    output_path=excel_path,\n",
    "    summary=summary,\n",
    "    elimination_results=elimination_results,\n",
    "    corr_pairs_df=corr_pairs_df,\n",
    "    selection_df=selection_df,\n",
    "    performance_df=performance_df,\n",
    "    lift_tables=lift_tables,\n",
    "    importance_df=importance_df,\n",
    "    vif_df=vif_result.details_df if vif_result else None,\n",
    "    tuning_df=tuning_df,\n",
    "    tuning_best_params=best_params,\n",
    "    chart_path=chart_path,\n",
    "    score_psi_df=score_psi_df,\n",
    "    bootstrap_df=bootstrap_df,\n",
    "    shap_summary_df=shap_summary,\n",
    "    shap_plot_path=shap_plot_paths[0] if shap_plot_paths else None,\n",
    "    calibration_dict=calibration_dict,\n",
    "    validation_report_df=validation_report_df,\n",
    ")\n",
    "\n",
    "# Step 9f: Save model artifact\n",
    "model_path = None\n",
    "if config.output.save_model:\n",
    "    model_path = output_manager.save_artifact(\"model\", final_model, fmt=\"joblib\")\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save run metadata\n",
    "output_manager.mark_complete(\"success\")\n",
    "if config.reproducibility.save_metadata:\n",
    "    output_manager.save_run_metadata()\n",
    "\n",
    "print(f\"\\nRun ID: {output_manager.run_id}\")\n",
    "print(f\"Excel report: {excel_path}\")\n",
    "if chart_path:\n",
    "    print(f\"Selection chart: {chart_path}\")\n",
    "if shap_plot_paths:\n",
    "    print(f\"SHAP plots: {shap_plot_paths}\")\n",
    "if has_critical_failures:\n",
    "    print(\"WARNING: Validation found critical failures!\")\n",
    "print(f\"All outputs: {output_manager.run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative: Full Pipeline Mode\n",
    "\n",
    "Run everything in one call using `ModelDevelopmentPipeline` — the exact same class used by the CLI script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the full pipeline in one call:\n",
    "\n",
    "# from src.config.loader import load_config, save_config\n",
    "# from src.io.output_manager import OutputManager\n",
    "# from src.model_development.pipeline import ModelDevelopmentPipeline\n",
    "# from src.model_development.eliminators import (\n",
    "#     QuarterlyPSICheck, YearlyPSICheck, ConsecutiveQuarterPSICheck,\n",
    "#     HalfSplitPSICheck, DateSplitPSICheck,\n",
    "# )\n",
    "#\n",
    "# config = load_config(\"config/model_development.yaml\")\n",
    "# output_manager = OutputManager(config)\n",
    "#\n",
    "# if config.reproducibility.save_config:\n",
    "#     save_config(config, str(output_manager.run_dir / \"config\" / \"pipeline_config.yaml\"))\n",
    "#\n",
    "# # Build PSI checks from config\n",
    "# check_map = {\"quarterly\": QuarterlyPSICheck, \"yearly\": YearlyPSICheck,\n",
    "#               \"consecutive\": ConsecutiveQuarterPSICheck, \"halfsplit\": HalfSplitPSICheck}\n",
    "# psi_checks = []\n",
    "# for c in config.steps.psi.checks:\n",
    "#     if c.type in check_map:\n",
    "#         psi_checks.append(check_map[c.type]())\n",
    "#     elif c.type == \"date_split\" and c.date:\n",
    "#         psi_checks.append(DateSplitPSICheck(c.date, label=c.label))\n",
    "#\n",
    "# pipeline = ModelDevelopmentPipeline(\n",
    "#     input_path=config.data.input_path,\n",
    "#     train_end_date=config.splitting.train_end_date,\n",
    "#     output_dir=str(output_manager.run_dir / \"reports\"),\n",
    "#     iv_min=config.steps.iv.min_iv,\n",
    "#     iv_max=config.steps.iv.max_iv,\n",
    "#     missing_threshold=config.steps.missing.threshold,\n",
    "#     psi_threshold=config.steps.psi.threshold,\n",
    "#     correlation_threshold=config.steps.correlation.threshold,\n",
    "#     test_size=config.splitting.test_size,\n",
    "#     target_column=config.data.target_column,\n",
    "#     date_column=config.data.date_column,\n",
    "#     xgb_params=config.model.params,\n",
    "#     psi_checks=psi_checks,\n",
    "#     selection_method=config.steps.selection.method,\n",
    "#     selection_cv=config.steps.selection.cv,\n",
    "#     selection_max_features=config.steps.selection.max_features,\n",
    "#     selection_min_features=config.steps.selection.min_features,\n",
    "#     selection_tolerance=config.steps.selection.tolerance,\n",
    "#     selection_patience=config.steps.selection.patience,\n",
    "#     vif_enabled=config.steps.vif.enabled,\n",
    "#     vif_threshold=config.steps.vif.threshold,\n",
    "#     vif_iv_aware=config.steps.vif.iv_aware,\n",
    "#     tuning_enabled=config.model.tuning.enabled,\n",
    "#     tuning_n_trials=config.model.tuning.n_trials,\n",
    "#     tuning_timeout=config.model.tuning.timeout,\n",
    "#     tuning_cv=config.model.tuning.cv,\n",
    "#     config=config,\n",
    "#     output_manager=output_manager,\n",
    "# )\n",
    "#\n",
    "# results = pipeline.run()\n",
    "#\n",
    "# output_manager.mark_complete(results.get(\"status\", \"unknown\"))\n",
    "# if config.reproducibility.save_metadata:\n",
    "#     output_manager.save_run_metadata()\n",
    "#\n",
    "# print(f\"Status: {results['status']}\")\n",
    "# print(f\"Selected features: {results['after_selection']}\")\n",
    "# print(f\"Excel: {results['excel_path']}\")\n",
    "# print(f\"Run dir: {output_manager.run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}