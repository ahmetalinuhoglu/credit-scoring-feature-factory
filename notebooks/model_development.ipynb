{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Model Development Pipeline\n",
    "\n",
    "Interactive notebook for step-by-step model development. Uses the same code paths as `scripts/run_model_development.py`.\n",
    "\n",
    "**Steps:**\n",
    "1. Load config and data, split into Train / Test / OOT\n",
    "2. Data quality checks\n",
    "3. Constant feature elimination\n",
    "4. Missing value elimination\n",
    "5. IV (Information Value) filtering\n",
    "6. PSI stability filtering\n",
    "7. Correlation elimination\n",
    "8. Sequential feature selection (forward/backward with CV)\n",
    "9. VIF multicollinearity check\n",
    "10. Hyperparameter tuning (Optuna)\n",
    "11. Model evaluation (quarterly metrics, lift tables, importance)\n",
    "12. Score PSI, Bootstrap CI, Calibration, SHAP, Validation\n",
    "13. Generate Excel report and save outputs\n",
    "\n",
    "Each cell is self-contained and re-runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/ahmetalinuhoglu/Documents/Personal/Projects/an-model-development\n",
      "Working dir:  /Users/ahmetalinuhoglu/Documents/Personal/Projects/an-model-development\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "project_root = str(Path.cwd().parent) if Path.cwd().name == \"notebooks\" else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-5s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "from src.config.loader import load_config, save_config\n",
    "from src.config.schema import PipelineConfig\n",
    "from src.io.output_manager import OutputManager\n",
    "from src.model_development.data_loader import load_and_split\n",
    "from src.model_development.eliminators import (\n",
    "    ConstantEliminator,\n",
    "    MissingEliminator,\n",
    "    IVEliminator,\n",
    "    PSIEliminator,\n",
    "    CorrelationEliminator,\n",
    "    VIFEliminator,\n",
    "    QuarterlyPSICheck,\n",
    "    YearlyPSICheck,\n",
    "    ConsecutiveQuarterPSICheck,\n",
    "    HalfSplitPSICheck,\n",
    "    DateSplitPSICheck,\n",
    ")\n",
    "from src.model_development.feature_selector import sequential_feature_selection\n",
    "from src.model_development.hyperparameter_tuner import tune_hyperparameters\n",
    "from src.model_development.evaluator import (\n",
    "    evaluate_model_quarterly,\n",
    "    bootstrap_auc_ci,\n",
    "    compute_score_psi,\n",
    ")\n",
    "from src.model_development import excel_reporter\n",
    "from src.validation.data_checks import DataValidator\n",
    "from src.validation.model_checks import ModelValidator\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working dir:  {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Config\n",
    "\n",
    "Single YAML config drives everything. Override values for this session without editing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:35:56 | INFO  | Loaded config from config/model_development.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:          data/sample/sample_features.parquet\n",
      "Train end date: 2024-06-30\n",
      "Target:         target\n",
      "Seed:           42\n",
      "n_jobs:         -1\n",
      "IV range:       [0.02, 0.5]\n",
      "PSI threshold:  0.25\n",
      "Corr threshold: 0.8\n",
      "Selection:      forward (max 20)\n",
      "VIF:            enabled (threshold 5.0)\n",
      "Tuning:         enabled (100 trials)\n",
      "Calibration:    enabled (platt)\n",
      "SHAP:           enabled\n",
      "Bootstrap CI:   enabled\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"config/model_development.yaml\")\n",
    "\n",
    "# Override for this session (uncomment and edit as needed):\n",
    "# config = load_config(\"config/model_development.yaml\", cli_overrides={\n",
    "#     \"splitting.train_end_date\": \"2024-06-30\",\n",
    "#     \"steps.iv.min_iv\": 0.03,\n",
    "#     \"model.params.max_depth\": 4,\n",
    "# })\n",
    "\n",
    "n_jobs = config.reproducibility.n_jobs\n",
    "\n",
    "print(f\"Input:          {config.data.input_path}\")\n",
    "print(f\"Train end date: {config.splitting.train_end_date}\")\n",
    "print(f\"Target:         {config.data.target_column}\")\n",
    "print(f\"Seed:           {config.reproducibility.global_seed}\")\n",
    "print(f\"n_jobs:         {n_jobs}\")\n",
    "print(f\"IV range:       [{config.steps.iv.min_iv}, {config.steps.iv.max_iv}]\")\n",
    "print(f\"PSI threshold:  {config.steps.psi.threshold}\")\n",
    "print(f\"Corr threshold: {config.steps.correlation.threshold}\")\n",
    "print(f\"Selection:      {config.steps.selection.method} (max {config.steps.selection.max_features})\")\n",
    "print(f\"VIF:            {'enabled' if config.steps.vif.enabled else 'disabled'} (threshold {config.steps.vif.threshold})\")\n",
    "print(f\"Tuning:         {'enabled' if config.model.tuning.enabled else 'disabled'} ({config.model.tuning.n_trials} trials)\")\n",
    "print(f\"Calibration:    {'enabled' if config.evaluation.calibration.enabled else 'disabled'} ({config.evaluation.calibration.method})\")\n",
    "print(f\"SHAP:           {'enabled' if config.evaluation.shap.enabled else 'disabled'}\")\n",
    "print(f\"Bootstrap CI:   {'enabled' if config.evaluation.bootstrap.enabled else 'disabled'}\")\n",
    "\n",
    "# Set seeds\n",
    "seed = config.reproducibility.global_seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data & Split\n",
    "\n",
    "Uses `load_and_split()` — the same function called by the script pipeline.\n",
    "Stratified random or temporal train/test split within training period. OOT auto-split by quarter after `train_end_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:35:56 | INFO  | Loading data from data/sample/sample_features.parquet\n",
      "04:35:57 | INFO  | Loaded 11,529 rows, 948 columns\n",
      "04:35:57 | INFO  | Train end date: 2024-06-30\n",
      "04:35:57 | INFO  | Training period: 9,342 rows (2022-01-01 to 2024-06-30)\n",
      "04:35:57 | INFO  | Feature columns: 943\n",
      "04:35:57 | INFO  | Train: 7,473 rows (bad rate: 17.48%)\n",
      "04:35:57 | INFO  | Test: 1,869 rows (bad rate: 17.50%)\n",
      "04:35:57 | INFO  | OOT 2024Q3: 916 rows (bad rate: 17.69%)\n",
      "04:35:57 | INFO  | OOT 2024Q4: 926 rows (bad rate: 16.31%)\n",
      "04:35:57 | INFO  | OOT 2025Q1: 345 rows (bad rate: 20.29%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 943\n",
      "Train: 7,473 rows, bad rate: 17.48%\n",
      "Test:  1,869 rows, bad rate: 17.50%\n",
      "OOT 2024Q3: 916 rows, bad rate: 17.69%\n",
      "OOT 2024Q4: 926 rows, bad rate: 16.31%\n",
      "OOT 2025Q1: 345 rows, bad rate: 20.29%\n"
     ]
    }
   ],
   "source": [
    "datasets = load_and_split(\n",
    "    input_path=config.data.input_path,\n",
    "    train_end_date=config.splitting.train_end_date,\n",
    "    target_column=config.data.target_column,\n",
    "    date_column=config.data.date_column,\n",
    "    test_size=config.splitting.test_size,\n",
    "    stratify=config.splitting.stratify,\n",
    ")\n",
    "\n",
    "target = config.data.target_column\n",
    "features = list(datasets.feature_columns)\n",
    "X_train = datasets.train[features]\n",
    "y_train = datasets.train[target]\n",
    "X_test = datasets.test[features]\n",
    "y_test = datasets.test[target]\n",
    "\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Train: {len(datasets.train):,} rows, bad rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test:  {len(datasets.test):,} rows, bad rate: {y_test.mean():.2%}\")\n",
    "for label in datasets.oot_labels:\n",
    "    qdf = datasets.oot_quarters[label]\n",
    "    print(f\"OOT {label}: {len(qdf):,} rows, bad rate: {qdf[target].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Automated pre-pipeline checks: target validation, date validation, duplicates, leakage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:35:58 | INFO  | DATA_CHECK | PASS | Non-empty dataset | Dataset has 11,529 rows.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Target column exists | Target column 'target' present.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Target is binary | Target is binary (0/1).\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Target has no nulls | No null values in target.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Bad rate within range | Bad rate 17.4863% is within acceptable range.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Date column exists | Date column 'application_date' present and parseable.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Date range coverage | Date range covers 13 quarters (2022-01-01 to 2025-01-31).\n",
      "04:35:58 | WARNING | DATA_CHECK | WARNING | Features are numeric | 1 non-numeric feature(s) found.\n",
      "04:35:58 | WARNING | DATA_CHECK | WARNING | No duplicate IDs | 1,529 duplicate application_id values found.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Sufficient sample size | 11,529 rows available.\n",
      "04:35:58 | INFO  | DATA_CHECK | PASS | Leakage detection | No features with suspiciously high AUC detected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Report: 9 PASS, 2 WARNING, 0 FAIL\n",
      "  [+] Non-empty dataset: Dataset has 11,529 rows.\n",
      "  [+] Target column exists: Target column 'target' present.\n",
      "  [+] Target is binary: Target is binary (0/1).\n",
      "  [+] Target has no nulls: No null values in target.\n",
      "  [+] Bad rate within range: Bad rate 17.4863% is within acceptable range.\n",
      "  [+] Date column exists: Date column 'application_date' present and parseable.\n",
      "  [+] Date range coverage: Date range covers 13 quarters (2022-01-01 to 2025-01-31).\n",
      "  [!] Features are numeric: 1 non-numeric feature(s) found.\n",
      "  [!] No duplicate IDs: 1,529 duplicate application_id values found.\n",
      "  [+] Sufficient sample size: 11,529 rows available.\n",
      "  [+] Leakage detection: No features with suspiciously high AUC detected.\n"
     ]
    }
   ],
   "source": [
    "# Load raw data for data validator (it expects the full DataFrame)\n",
    "if config.data.input_path.endswith('.csv'):\n",
    "    df_raw = pd.read_csv(config.data.input_path)\n",
    "else:\n",
    "    df_raw = pd.read_parquet(config.data.input_path)\n",
    "\n",
    "data_validator = DataValidator(config)\n",
    "data_report = data_validator.validate(df_raw)\n",
    "print(data_report.summary())\n",
    "\n",
    "if data_report.has_critical_failures:\n",
    "    print(\"\\nCRITICAL FAILURES — review before proceeding:\")\n",
    "    for check in data_report.checks:\n",
    "        if check.status.value == \"FAIL\":\n",
    "            print(f\"  FAIL: {check.check_name} — {check.message}\")\n",
    "            print(f\"        Fix: {check.recommendation}\")\n",
    "\n",
    "del df_raw  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Steps\n",
    "\n",
    "Each step narrows the feature set. Run cells in order.\n",
    "\n",
    "Elimination results are collected for the Excel report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Constant Elimination\n",
    "Remove features with fewer than 2 distinct values (zero variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:00 | INFO  | CONSTANT | Eliminated 131 features (812 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Constant: 943 -> 812 features (131 eliminated)\n",
      "\n",
      "Eliminated features (131):\n",
      "                                       Feature  Unique_Count\n",
      "                     co_applicant_default_rate             1\n",
      "                            moved_to_unsecured             1\n",
      " installment_sale_defaulted_amount_std_last_6m             1\n",
      "             mortgage_recovered_count_last_24m             1\n",
      "      mortgage_recovered_total_amount_last_24m             1\n",
      "    mortgage_recovered_average_amount_last_24m             1\n",
      "installment_loan_recovered_amount_std_last_12m             1\n",
      "        mortgage_recovered_max_amount_last_24m             1\n",
      "         mortgage_recovered_amount_std_last_3m             1\n",
      "         mortgage_recovered_min_amount_last_3m             1\n"
     ]
    }
   ],
   "source": [
    "elimination_results = []\n",
    "\n",
    "const_elim = ConstantEliminator()\n",
    "const_result = const_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(const_result)\n",
    "features = const_result.kept_features\n",
    "\n",
    "print(f\"{const_result.step_name}: {const_result.n_kept + const_result.n_eliminated} -> \"\n",
    "      f\"{const_result.n_kept} features ({const_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if const_result.eliminated_features:\n",
    "    elim_df = const_result.details_df[const_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nEliminated features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Unique_Count\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Missing Elimination\n",
    "Remove features with missing rate above threshold on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:00 | INFO  | MISSING | Eliminated 0 features (812 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_Missing: 812 -> 812 features (0 eliminated)\n"
     ]
    }
   ],
   "source": [
    "missing_elim = MissingEliminator(max_missing_rate=config.steps.missing.threshold)\n",
    "missing_result = missing_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(missing_result)\n",
    "features = missing_result.kept_features\n",
    "\n",
    "print(f\"{missing_result.step_name}: {missing_result.n_kept + missing_result.n_eliminated} -> \"\n",
    "      f\"{missing_result.n_kept} features ({missing_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if missing_result.eliminated_features:\n",
    "    elim_df = missing_result.details_df[missing_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nEliminated features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Missing_Rate\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: IV Elimination\n",
    "Remove features with IV below `min_iv` (useless) or above `max_iv` (suspicious leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:03 | INFO  | IV | Eliminated 657 features (155 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_IV_Analysis: 812 -> 155 features (657 eliminated)\n",
      "\n",
      "IV Category Distribution:\n",
      "IV_Category\n",
      "useless       623\n",
      "weak           90\n",
      "medium         63\n",
      "suspicious     34\n",
      "strong          2\n",
      "\n",
      "Top 10 features by IV:\n",
      "                        Feature  IV_Score IV_Category  Univariate_AUC\n",
      "       is_monthly_payment_total    0.3401      strong          0.6298\n",
      "             default_count_ever    0.3026      strong          0.8466\n",
      "  installment_sale_total_amount    0.2941      medium          0.6280\n",
      "installment_sale_average_amount    0.2933      medium          0.6272\n",
      "                  is_avg_amount    0.2933      medium          0.6272\n",
      "    installment_sale_max_amount    0.2929      medium          0.6279\n",
      "    installment_sale_min_amount    0.2703      medium          0.6264\n",
      "      payment_to_exposure_ratio    0.2222      medium          0.6350\n",
      "             total_credit_count    0.2175      medium          0.6247\n",
      "    multi_product_default_count    0.2159      medium          0.8458\n"
     ]
    }
   ],
   "source": [
    "iv_elim = IVEliminator(min_iv=config.steps.iv.min_iv, max_iv=config.steps.iv.max_iv, n_jobs=n_jobs)\n",
    "iv_result = iv_elim.eliminate(X_train, y_train, features)\n",
    "elimination_results.append(iv_result)\n",
    "features = iv_result.kept_features\n",
    "\n",
    "# Extract IV scores for downstream use (correlation, selection, VIF)\n",
    "iv_scores = {}\n",
    "for _, row in iv_result.details_df.iterrows():\n",
    "    if row.get(\"IV_Score\") is not None:\n",
    "        iv_scores[row[\"Feature\"]] = row[\"IV_Score\"]\n",
    "\n",
    "print(f\"{iv_result.step_name}: {iv_result.n_kept + iv_result.n_eliminated} -> \"\n",
    "      f\"{iv_result.n_kept} features ({iv_result.n_eliminated} eliminated)\")\n",
    "\n",
    "# IV distribution\n",
    "print(f\"\\nIV Category Distribution:\")\n",
    "print(iv_result.details_df[\"IV_Category\"].value_counts().to_string())\n",
    "\n",
    "# Top features by IV\n",
    "kept_df = iv_result.details_df[iv_result.details_df[\"Status\"] == \"Kept\"].sort_values(\"IV_Score\", ascending=False)\n",
    "print(f\"\\nTop 10 features by IV:\")\n",
    "print(kept_df[[\"Feature\", \"IV_Score\", \"IV_Category\", \"Univariate_AUC\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: PSI Stability Elimination\n",
    "Remove features with unstable distributions within training data. PSI checks are built from config (quarterly, yearly, consecutive, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:03 | INFO  | PSI | Check QuarterlyPSICheck: 10 comparison(s)\n",
      "04:36:03 | INFO  | PSI | Check YearlyPSICheck: 3 comparison(s)\n",
      "04:36:03 | INFO  | PSI | Check ConsecutiveQuarterPSICheck: 9 comparison(s)\n",
      "04:36:03 | INFO  | PSI | Checking 155 features across 22 comparisons\n",
      "04:36:03 | INFO  | PSI | Eliminated 0 features (155 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_PSI_Stability: 155 -> 155 features (0 eliminated)\n"
     ]
    }
   ],
   "source": [
    "# Build PSI checks from config (same as run_model_development.py)\n",
    "check_map = {\n",
    "    \"quarterly\": QuarterlyPSICheck,\n",
    "    \"yearly\": YearlyPSICheck,\n",
    "    \"consecutive\": ConsecutiveQuarterPSICheck,\n",
    "    \"halfsplit\": HalfSplitPSICheck,\n",
    "}\n",
    "psi_checks = []\n",
    "for c in config.steps.psi.checks:\n",
    "    if c.type in check_map:\n",
    "        psi_checks.append(check_map[c.type]())\n",
    "    elif c.type == \"date_split\" and c.date:\n",
    "        psi_checks.append(DateSplitPSICheck(c.date, label=c.label))\n",
    "\n",
    "psi_elim = PSIEliminator(\n",
    "    critical_threshold=config.steps.psi.threshold,\n",
    "    checks=psi_checks,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "psi_result = psi_elim.eliminate(\n",
    "    X_train, y_train, features,\n",
    "    train_dates=datasets.train[config.data.date_column],\n",
    ")\n",
    "elimination_results.append(psi_result)\n",
    "features = psi_result.kept_features\n",
    "\n",
    "print(f\"{psi_result.step_name}: {psi_result.n_kept + psi_result.n_eliminated} -> \"\n",
    "      f\"{psi_result.n_kept} features ({psi_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if psi_result.eliminated_features:\n",
    "    elim_df = psi_result.details_df[psi_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "    print(f\"\\nUnstable features ({len(elim_df)}):\")\n",
    "    print(elim_df[[\"Feature\", \"Max_PSI\", \"Mean_PSI\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Correlation Elimination\n",
    "Greedy removal: among correlated pairs, keep the higher-IV feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:04 | INFO  | CORRELATION | Computing pearson correlation matrix for 155 features\n",
      "04:36:04 | INFO  | CORRELATION | Eliminated 74 features (81 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05_Correlation: 155 -> 81 features (74 eliminated)\n",
      "\n",
      "Correlated pairs (179):\n",
      "                      Feature_A                       Feature_B  Correlation                                                                    Decision\n",
      "             default_count_ever     multi_product_default_count       0.9762                multi_product_default_count eliminated by default_count_ever\n",
      "  installment_sale_total_amount installment_sale_average_amount       0.9570 installment_sale_average_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount                   is_avg_amount       0.9570                   is_avg_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount     installment_sale_max_amount       0.9798     installment_sale_max_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount     installment_sale_min_amount       0.9109     installment_sale_min_amount eliminated by installment_sale_total_amount\n",
      "  installment_sale_total_amount          installment_sale_count       0.8172          installment_sale_count eliminated by installment_sale_total_amount\n",
      "installment_sale_average_amount                   is_avg_amount       1.0000                                                             both eliminated\n",
      "installment_sale_average_amount     installment_sale_max_amount       0.9890                                                             both eliminated\n",
      "installment_sale_average_amount     installment_sale_min_amount       0.9880                                                             both eliminated\n",
      "                  is_avg_amount     installment_sale_max_amount       0.9890                                                             both eliminated\n"
     ]
    }
   ],
   "source": [
    "corr_elim = CorrelationEliminator(max_correlation=config.steps.correlation.threshold)\n",
    "corr_result = corr_elim.eliminate(X_train, y_train, features, iv_scores=iv_scores)\n",
    "elimination_results.append(corr_result)\n",
    "features = corr_result.kept_features\n",
    "\n",
    "# Save correlation pairs for the Excel report\n",
    "corr_pairs_df = getattr(corr_elim, \"corr_pairs_df\", None)\n",
    "\n",
    "print(f\"{corr_result.step_name}: {corr_result.n_kept + corr_result.n_eliminated} -> \"\n",
    "      f\"{corr_result.n_kept} features ({corr_result.n_eliminated} eliminated)\")\n",
    "\n",
    "if corr_pairs_df is not None and len(corr_pairs_df) > 0:\n",
    "    print(f\"\\nCorrelated pairs ({len(corr_pairs_df)}):\")\n",
    "    print(corr_pairs_df[[\"Feature_A\", \"Feature_B\", \"Correlation\", \"Decision\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sequential Feature Selection\n",
    "CV-based forward (or backward) selection with elbow detection (1-SE rule).\n",
    "Saves a performance chart to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:12 | INFO  | Output directory: outputs/model_development/20260212_043612_1dc021\n",
      "04:36:12 | INFO  | Config saved to outputs/model_development/20260212_043612_1dc021/config/pipeline_config.yaml\n",
      "04:36:12 | INFO  | SELECTION | Starting forward selection with 81 candidates, cv=5, max_features=20, tolerance=0.001, patience=3, n_jobs=-1\n",
      "04:36:12 | INFO  | SELECTION | Forward step 1/20: evaluating 81 candidates...\n",
      "04:36:13 | INFO  | SELECTION | Step 1: ADDED default_count_ever (IV=0.3026), CV AUC=0.8466 ± 0.0131\n",
      "04:36:13 | INFO  | SELECTION | Forward step 2/20: evaluating 80 candidates...\n",
      "04:36:13 | INFO  | SELECTION | Step 2: ADDED recovered_total_amount (IV=0.1737), CV AUC=0.8626 ± 0.0120\n",
      "04:36:13 | INFO  | SELECTION | Forward step 3/20: evaluating 79 candidates...\n",
      "04:36:14 | INFO  | SELECTION | Step 3: ADDED newest_credit_age_months (IV=0.0238), CV AUC=0.8730 ± 0.0056\n",
      "04:36:14 | INFO  | SELECTION | Forward step 4/20: evaluating 78 candidates...\n",
      "04:36:15 | INFO  | SELECTION | Step 4: ADDED avg_credit_age_months (IV=0.0761), CV AUC=0.8743 ± 0.0095\n",
      "04:36:15 | INFO  | SELECTION | Forward step 5/20: evaluating 77 candidates...\n",
      "04:36:16 | INFO  | SELECTION | Step 5: ADDED single_product_concentration (IV=0.0914), CV AUC=0.8763 ± 0.0042\n",
      "04:36:16 | INFO  | SELECTION | Forward step 6/20: evaluating 76 candidates...\n",
      "04:36:17 | INFO  | SELECTION | Step 6: ADDED min_single_credit_amount (IV=0.0949), CV AUC=0.8754 ± 0.0103\n",
      "04:36:17 | INFO  | SELECTION | Forward step 7/20: evaluating 75 candidates...\n",
      "04:36:19 | INFO  | SELECTION | Step 7: ADDED distinct_product_count (IV=0.2091), CV AUC=0.8757 ± 0.0096\n",
      "04:36:19 | INFO  | SELECTION | Forward step 8/20: evaluating 74 candidates...\n",
      "04:36:20 | INFO  | SELECTION | Step 8: ADDED installment_sale_total_amount (IV=0.2941), CV AUC=0.8786 ± 0.0090\n",
      "04:36:20 | INFO  | SELECTION | Forward step 9/20: evaluating 73 candidates...\n",
      "04:36:22 | INFO  | SELECTION | Step 9: ADDED std_inter_credit_days (IV=0.1593), CV AUC=0.8783 ± 0.0080\n",
      "04:36:22 | INFO  | SELECTION | Forward step 10/20: evaluating 72 candidates...\n",
      "04:36:24 | INFO  | SELECTION | Step 10: ADDED amount_regularity (IV=0.1247), CV AUC=0.8758 ± 0.0103\n",
      "04:36:24 | INFO  | SELECTION | Forward step 11/20: evaluating 71 candidates...\n",
      "04:36:26 | INFO  | SELECTION | Step 11: ADDED amortizing_ratio (IV=0.1288), CV AUC=0.8774 ± 0.0100\n",
      "04:36:26 | INFO  | SELECTION | Early stopping at step 11 (no improvement for 3 steps)\n",
      "04:36:26 | INFO  | SELECTION | Optimal feature count: 3 (1-SE rule)\n",
      "04:36:26 | INFO  | SELECTION | Selected 3 features: ['default_count_ever', 'recovered_total_amount', 'newest_credit_age_months']\n",
      "04:36:26 | INFO  | SELECTION | Performance chart saved to outputs/model_development/20260212_043612_1dc021/reports/selection_chart_20260212_043626.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (3):\n",
      "  1. default_count_ever (IV=0.3026)\n",
      "  2. recovered_total_amount (IV=0.1737)\n",
      "  3. newest_credit_age_months (IV=0.0238)\n",
      "\n",
      "Selection chart: outputs/model_development/20260212_043612_1dc021/reports/selection_chart_20260212_043626.png\n",
      "\n",
      "Selection steps:\n",
      " Step  N_Features  Mean_CV_AUC  Std_CV_AUC  Is_Optimal\n",
      "    1           1     0.846575    0.013117       False\n",
      "    2           2     0.862620    0.011953       False\n",
      "    3           3     0.872956    0.005596        True\n",
      "    4           4     0.874280    0.009492       False\n",
      "    5           5     0.876345    0.004179       False\n",
      "    6           6     0.875424    0.010323       False\n",
      "    7           7     0.875699    0.009609       False\n",
      "    8           8     0.878594    0.008959       False\n",
      "    9           9     0.878309    0.008018       False\n",
      "   10          10     0.875838    0.010283       False\n",
      "   11          11     0.877377    0.009961       False\n"
     ]
    }
   ],
   "source": [
    "# Create output manager for saving outputs\n",
    "output_manager = OutputManager(config)\n",
    "\n",
    "# Save config snapshot\n",
    "if config.reproducibility.save_config:\n",
    "    save_config(config, str(output_manager.run_dir / \"config\" / \"pipeline_config.yaml\"))\n",
    "\n",
    "# Selection chart goes into the run's reports directory\n",
    "selection_output_dir = str(output_manager.run_dir / \"reports\")\n",
    "\n",
    "selected_features, selection_df, chart_path = sequential_feature_selection(\n",
    "    X_train=X_train[features],\n",
    "    y_train=y_train,\n",
    "    X_test=X_test[features],\n",
    "    y_test=y_test,\n",
    "    features=features,\n",
    "    direction=config.steps.selection.method,\n",
    "    cv=config.steps.selection.cv,\n",
    "    min_features=config.steps.selection.min_features,\n",
    "    max_features=config.steps.selection.max_features,\n",
    "    tolerance=config.steps.selection.tolerance,\n",
    "    patience=config.steps.selection.patience,\n",
    "    iv_scores=iv_scores,\n",
    "    xgb_params=config.model.params,\n",
    "    n_jobs=n_jobs,\n",
    "    output_dir=selection_output_dir,\n",
    ")\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    iv = iv_scores.get(feat, 0)\n",
    "    print(f\"  {i}. {feat} (IV={iv:.4f})\")\n",
    "\n",
    "print(f\"\\nSelection chart: {chart_path}\")\n",
    "\n",
    "# Show selection steps\n",
    "added = selection_df[selection_df.get(\"Is_Optimal\", False) | True]  # show all steps\n",
    "print(f\"\\nSelection steps:\")\n",
    "cols = [c for c in [\"Step\", \"N_Features\", \"Mean_CV_AUC\", \"Std_CV_AUC\", \"Is_Optimal\"] if c in selection_df.columns]\n",
    "print(selection_df[cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: VIF Multicollinearity Check\n",
    "Post-selection VIF check. Iteratively removes features with VIF above threshold, preserving higher-IV features when `iv_aware=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:30 | INFO  | VIF | Starting VIF check with 3 features, threshold=5.0\n",
      "04:36:30 | INFO  | VIF | Iteration 1: max VIF=1.1548, all features below threshold\n",
      "04:36:30 | INFO  | VIF | Eliminated 0 features (3 remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07_VIF: 3 -> 3 features (0 eliminated)\n",
      "All features passed VIF check.\n",
      "                 Feature  VIF_Initial  VIF_Final  IV_Score\n",
      "      default_count_ever       1.1548     1.1548    0.3026\n",
      "  recovered_total_amount       1.1508     1.1508    0.1737\n",
      "newest_credit_age_months       1.0046     1.0046    0.0238\n",
      "\n",
      "Final selected features (3): ['default_count_ever', 'recovered_total_amount', 'newest_credit_age_months']\n"
     ]
    }
   ],
   "source": [
    "vif_result = None\n",
    "if config.steps.vif.enabled and len(selected_features) > 2:\n",
    "    vif_elim = VIFEliminator(\n",
    "        threshold=config.steps.vif.threshold,\n",
    "        iv_aware=config.steps.vif.iv_aware,\n",
    "    )\n",
    "    vif_result = vif_elim.eliminate(\n",
    "        X_train, y_train, selected_features, iv_scores=iv_scores,\n",
    "    )\n",
    "    elimination_results.append(vif_result)\n",
    "    selected_features = vif_result.kept_features\n",
    "\n",
    "    print(f\"{vif_result.step_name}: {vif_result.n_kept + vif_result.n_eliminated} -> \"\n",
    "          f\"{vif_result.n_kept} features ({vif_result.n_eliminated} eliminated)\")\n",
    "\n",
    "    if vif_result.n_eliminated > 0:\n",
    "        elim_df = vif_result.details_df[vif_result.details_df[\"Status\"] == \"Eliminated\"]\n",
    "        print(f\"\\nEliminated by VIF:\")\n",
    "        print(elim_df[[\"Feature\", \"VIF_Initial\", \"IV_Score\", \"Elimination_Round\"]].to_string(index=False))\n",
    "    else:\n",
    "        print(\"All features passed VIF check.\")\n",
    "        print(vif_result.details_df[[\"Feature\", \"VIF_Initial\", \"VIF_Final\", \"IV_Score\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"VIF check skipped (disabled or too few features)\")\n",
    "\n",
    "print(f\"\\nFinal selected features ({len(selected_features)}): {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Hyperparameter Tuning (Optuna)\n",
    "Optuna-based Bayesian optimization with TPE sampler and stratified CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:33 | INFO  | TUNING | Starting Optuna tuning — stability-aware, 100 trials, n_jobs=-1, stability_weight=1.0\n",
      "04:36:33 | INFO  | TUNING | scale_pos_weight=4.7221 (neg=6167, pos=1306)\n",
      "04:36:33 | INFO  | TUNING | Trial 7: Score=0.8374 (Mean=0.8497, Std=0.0123) [Train=0.8484, Test=0.8401, OOT_2024Q3=0.8559, OOT_2024Q4=0.8345, OOT_2025Q1=0.8696] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 5: Score=0.8389 (Mean=0.8503, Std=0.0115) [Train=0.8507, Test=0.8404, OOT_2024Q3=0.8568, OOT_2024Q4=0.8359, OOT_2025Q1=0.8679] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 1: Score=0.8574 (Mean=0.8676, Std=0.0102) [Train=0.8835, Test=0.8640, OOT_2024Q3=0.8734, OOT_2024Q4=0.8531, OOT_2025Q1=0.8643] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 11: Score=0.8580 (Mean=0.8701, Std=0.0121) [Train=0.8781, Test=0.8651, OOT_2024Q3=0.8741, OOT_2024Q4=0.8492, OOT_2025Q1=0.8838] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 0: Score=0.8565 (Mean=0.8691, Std=0.0126) [Train=0.8841, Test=0.8650, OOT_2024Q3=0.8767, OOT_2024Q4=0.8472, OOT_2025Q1=0.8723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Optuna tuning: 100 trials, stability_weight=1.0, n_jobs=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:33 | INFO  | TUNING | Trial 10: Score=0.8586 (Mean=0.8699, Std=0.0113) [Train=0.8769, Test=0.8667, OOT_2024Q3=0.8769, OOT_2024Q4=0.8491, OOT_2025Q1=0.8799] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 14: Score=0.8587 (Mean=0.8700, Std=0.0112) [Train=0.8794, Test=0.8635, OOT_2024Q3=0.8750, OOT_2024Q4=0.8511, OOT_2025Q1=0.8809] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 20: Score=0.8586 (Mean=0.8684, Std=0.0098) [Train=0.8814, Test=0.8665, OOT_2024Q3=0.8753, OOT_2024Q4=0.8524, OOT_2025Q1=0.8665]\n",
      "04:36:33 | INFO  | TUNING | Trial 32: Score=0.8604 (Mean=0.8702, Std=0.0097) [Train=0.8740, Test=0.8678, OOT_2024Q3=0.8813, OOT_2024Q4=0.8526, OOT_2025Q1=0.8750] (new best)\n",
      "04:36:33 | INFO  | TUNING | Trial 30: Score=0.8505 (Mean=0.8704, Std=0.0200) [Train=0.9027, Test=0.8599, OOT_2024Q3=0.8816, OOT_2024Q4=0.8448, OOT_2025Q1=0.8631]\n",
      "04:36:33 | INFO  | TUNING | Trial 40: Score=0.8559 (Mean=0.8713, Std=0.0155) [Train=0.8960, Test=0.8604, OOT_2024Q3=0.8802, OOT_2024Q4=0.8520, OOT_2025Q1=0.8681]\n",
      "04:36:34 | INFO  | TUNING | Trial 50: Score=0.8575 (Mean=0.8688, Std=0.0113) [Train=0.8717, Test=0.8656, OOT_2024Q3=0.8795, OOT_2024Q4=0.8486, OOT_2025Q1=0.8786]\n",
      "04:36:34 | INFO  | TUNING | Trial 58: Score=0.8608 (Mean=0.8703, Std=0.0095) [Train=0.8829, Test=0.8662, OOT_2024Q3=0.8768, OOT_2024Q4=0.8550, OOT_2025Q1=0.8707] (new best)\n",
      "04:36:34 | INFO  | TUNING | Trial 60: Score=0.8609 (Mean=0.8699, Std=0.0090) [Train=0.8834, Test=0.8678, OOT_2024Q3=0.8743, OOT_2024Q4=0.8557, OOT_2025Q1=0.8683] (new best)\n",
      "04:36:34 | INFO  | TUNING | Trial 70: Score=0.8579 (Mean=0.8680, Std=0.0101) [Train=0.8726, Test=0.8657, OOT_2024Q3=0.8768, OOT_2024Q4=0.8493, OOT_2025Q1=0.8756]\n",
      "04:36:34 | INFO  | TUNING | Trial 80: Score=0.8572 (Mean=0.8694, Std=0.0121) [Train=0.8808, Test=0.8640, OOT_2024Q3=0.8794, OOT_2024Q4=0.8482, OOT_2025Q1=0.8744]\n",
      "04:36:34 | INFO  | TUNING | Trial 90: Score=0.8560 (Mean=0.8688, Std=0.0129) [Train=0.8843, Test=0.8642, OOT_2024Q3=0.8746, OOT_2024Q4=0.8464, OOT_2025Q1=0.8747]\n",
      "04:36:34 | INFO  | TUNING | Best trial: #60, Score = 0.8609\n",
      "04:36:34 | INFO  | TUNING | Best params: max_depth=6, learning_rate=0.0867, subsample=0.9522, colsample_bytree=0.9172, min_child_weight=44, gamma=1.7096, reg_alpha=0.2451, reg_lambda=0.0010\n",
      "04:36:34 | INFO  | TUNING | Best trial AUCs: AUC_Train=0.8834, AUC_Test=0.8678, AUC_OOT_2024Q3=0.8743, AUC_OOT_2024Q4=0.8557, AUC_OOT_2025Q1=0.8683, AUC_Mean=0.8699, AUC_Std=0.009\n",
      "04:36:34 | INFO  | TUNING | Training final model with best params...\n",
      "04:36:34 | INFO  | TUNING | Final model test AUC: 0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params: {'max_depth': 6, 'learning_rate': 0.08669738540392355, 'subsample': 0.9521916554373138, 'colsample_bytree': 0.9171587552444311, 'min_child_weight': 44, 'gamma': 1.709553907594803, 'reg_alpha': 0.2450903148386696, 'reg_lambda': 0.001048827151275742, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'verbosity': 0, 'n_jobs': -1, 'random_state': 42, 'early_stopping_rounds': 30, 'scale_pos_weight': 4.722052067381317, 'n_estimators': 1000}\n",
      "\n",
      "Trial history (100 trials):\n",
      " Trial  max_depth  learning_rate  n_estimators  subsample  colsample_bytree  min_child_weight    gamma  reg_alpha  reg_lambda  AUC_Train  AUC_Test  AUC_OOT_2024Q3  AUC_OOT_2024Q4  AUC_OOT_2025Q1  AUC_Mean  AUC_Std  Score  Duration_Sec\n",
      "     0          5       0.034323          1000   0.886774          0.815078                21 0.732054   0.279215    0.047655     0.8841    0.8650          0.8767          0.8472          0.8723    0.8691   0.0126 0.8565           0.2\n",
      "     1          6       0.074287          1000   0.973595          0.959902                30 1.942471   0.009858    0.224483     0.8835    0.8640          0.8734          0.8531          0.8643    0.8676   0.0102 0.8574           0.2\n",
      "     2          8       0.007122          1000   0.820680          0.778568                18 1.969427   0.035723    0.144110     0.8894    0.8611          0.8760          0.8502          0.8688    0.8691   0.0133 0.8558           0.2\n",
      "     3          4       0.064340          1000   0.855411          0.618776                21 4.943453   5.172752    0.047928     0.8509    0.8404          0.8575          0.8341          0.8682    0.8502   0.0121 0.8381           0.1\n",
      "     4          6       0.015385          1000   0.948643          0.737611                20 0.896661   4.804526    0.002830     0.8720    0.8641          0.8785          0.8469          0.8768    0.8677   0.0115 0.8561           0.2\n",
      "     5          4       0.006400          1000   0.907826          0.534061                17 3.355230   0.080335    6.851747     0.8507    0.8404          0.8568          0.8359          0.8679    0.8503   0.0115 0.8389           0.1\n",
      "     6          3       0.014370          1000   0.787002          0.906581                25 4.564294   0.779293    0.112390     0.8715    0.8648          0.8845          0.8513          0.8739    0.8692   0.0110 0.8582           0.2\n",
      "     7          3       0.034337          1000   0.874890          0.590883                29 4.199096   1.952735    0.003698     0.8484    0.8401          0.8559          0.8345          0.8696    0.8497   0.0123 0.8374           0.1\n",
      "     8          7       0.015831          1000   0.786886          0.977710                37 1.341702   0.221829    0.046065     0.8857    0.8644          0.8771          0.8474          0.8671    0.8684   0.0129 0.8554           0.2\n",
      "     9          6       0.042191          1000   0.702288          0.925832                 9 1.694529   0.815332    0.088892     0.8870    0.8640          0.8802          0.8485          0.8594    0.8678   0.0140 0.8538           0.2\n"
     ]
    }
   ],
   "source": [
    "tuning_df = None\n",
    "best_params = None\n",
    "\n",
    "if config.model.tuning.enabled:\n",
    "    print(f\"Running Optuna tuning: {config.model.tuning.n_trials} trials, \"\n",
    "          f\"stability_weight={config.model.tuning.stability_weight}, n_jobs={n_jobs}\")\n",
    "    best_params, tuning_df, final_model = tune_hyperparameters(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        features=selected_features,\n",
    "        n_trials=config.model.tuning.n_trials,\n",
    "        timeout=config.model.tuning.timeout,\n",
    "        cv=config.model.tuning.cv,\n",
    "        n_jobs=n_jobs,\n",
    "        oot_quarters=datasets.oot_quarters,\n",
    "        target_column=config.data.target_column,\n",
    "        stability_weight=config.model.tuning.stability_weight,\n",
    "    )\n",
    "    print(f\"\\nBest params: {best_params}\")\n",
    "    if tuning_df is not None:\n",
    "        print(f\"\\nTrial history ({len(tuning_df)} trials):\")\n",
    "        print(tuning_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"Tuning disabled — training with default/configured params\")\n",
    "    # Train with configured params (same as pipeline._train_default_model)\n",
    "    params = (config.model.params or {}).copy()\n",
    "    if not params:\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"n_estimators\": 300,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "    # Auto-balance\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    if params.pop(\"scale_pos_weight\", None) == \"auto\":\n",
    "        params[\"scale_pos_weight\"] = neg_count / pos_count\n",
    "    # early_stopping_rounds to constructor (xgboost >= 2.0)\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\", 30)\n",
    "    params[\"early_stopping_rounds\"] = early_stopping_rounds\n",
    "\n",
    "    final_model = xgb.XGBClassifier(**params)\n",
    "    final_model.fit(\n",
    "        X_train[selected_features], y_train,\n",
    "        eval_set=[(X_test[selected_features], y_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f\"Model trained with {final_model.n_estimators} estimators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Model Evaluation\n",
    "Evaluate the final model on Train, Test, and each OOT quarter. Produces performance table, lift tables, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:37 | INFO  | OOT | Train: AUC=0.8834, Gini=0.7668, KS=0.6895\n",
      "04:36:37 | INFO  | OOT | Test: AUC=0.8678, Gini=0.7357, KS=0.6831\n",
      "04:36:37 | INFO  | OOT | OOT_2024Q3: AUC=0.8743, Gini=0.7486, KS=0.7139\n",
      "04:36:37 | INFO  | OOT | OOT_2024Q4: AUC=0.8557, Gini=0.7115, KS=0.6589\n",
      "04:36:37 | INFO  | OOT | OOT_2025Q1: AUC=0.8683, Gini=0.7367, KS=0.7452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by period:\n",
      "    Period  N_Samples  N_Bads  Bad_Rate    AUC   Gini     KS  Precision_at_10pct  Lift_at_10pct\n",
      "     Train       7473    1306    0.1748 0.8834 0.7668 0.6895              0.7336           4.20\n",
      "      Test       1869     327    0.1750 0.8678 0.7357 0.6831              0.6613           3.78\n",
      "OOT_2024Q3        916     162    0.1769 0.8743 0.7486 0.7139              0.7692           4.35\n",
      "OOT_2024Q4        926     151    0.1631 0.8557 0.7115 0.6589              0.6957           4.27\n",
      "OOT_2025Q1        345      70    0.2029 0.8683 0.7367 0.7452              0.8529           4.20\n",
      "\n",
      "Feature importance:\n",
      "                 Feature  Importance  Rank  Cumulative_Importance\n",
      "      default_count_ever    0.944123     1               0.944123\n",
      "  recovered_total_amount    0.047945     2               0.992068\n",
      "newest_credit_age_months    0.007932     3               1.000000\n"
     ]
    }
   ],
   "source": [
    "performance_df, lift_tables, importance_df = evaluate_model_quarterly(\n",
    "    model=final_model,\n",
    "    selected_features=selected_features,\n",
    "    train_df=datasets.train,\n",
    "    test_df=datasets.test,\n",
    "    oot_quarters=datasets.oot_quarters,\n",
    "    target_column=config.data.target_column,\n",
    "    importance_type=config.evaluation.importance_type,\n",
    ")\n",
    "\n",
    "print(\"Performance by period:\")\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nFeature importance:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Tables\n",
    "Decile-based lift tables for each evaluation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Lift Table: Train\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7523     0.7833      0.7592    748   548    0.7326 4.191975        748       548      0.732620  4.192092      0.419602\n",
      "     2     0.6553     0.7523      0.7211    747   417    0.5582 3.194049       1495       965      0.645485  3.693498      0.738897\n",
      "     3     0.3136     0.6553      0.4810    747   133    0.1780 1.018525       2242      1098      0.489741  2.802325      0.840735\n",
      "     4     0.2987     0.3136      0.3050    747    46    0.0616 0.352478       2989      1144      0.382737  2.190039      0.875957\n",
      "     5     0.2985     0.2987      0.2985    747    39    0.0522 0.298691       3736      1183      0.316649  1.811881      0.905819\n",
      "     6     0.2889     0.2985      0.2926    748    29    0.0388 0.222016       4484      1212      0.270294  1.546639      0.928025\n",
      "     7     0.2889     0.2889      0.2889    747    28    0.0375 0.214577       5231      1240      0.237048  1.356403      0.949464\n",
      "     8     0.2834     0.2889      0.2862    747    28    0.0375 0.214577       5978      1268      0.212111  1.213711      0.970904\n",
      "     9     0.2784     0.2834      0.2807    747    26    0.0348 0.199127       6725      1294      0.192416  1.101016      0.990812\n",
      "    10     0.2474     0.2784      0.2620    748    12    0.0160 0.091553       7473      1306      0.174762  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: Test\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7548     0.7817      0.7605    161   107    0.6646 3.798585        161       107      0.664596  3.798564      0.327217\n",
      "     2     0.6780     0.7523      0.7392    213   130    0.6103 3.488228        374       237      0.633690  3.621915      0.724771\n",
      "     3     0.3203     0.6767      0.5758    181    40    0.2210 1.263147        555       277      0.499099  2.852649      0.847095\n",
      "     4     0.3003     0.3156      0.3104    160     7    0.0438 0.250343        715       284      0.397203  2.270251      0.868502\n",
      "     5     0.2987     0.3001      0.2993     89     3    0.0337 0.192616        804       287      0.356965  2.040269      0.877676\n",
      "     6     0.2910     0.2985      0.2957    317    12    0.0379 0.216621       1121       299      0.266726  1.524499      0.914373\n",
      "     7        NaN        NaN         NaN      0     0       NaN      NaN       1121       299      0.266726  1.524499      0.914373\n",
      "     8     0.2837     0.2889      0.2879    338    12    0.0355 0.202904       1459       311      0.213160  1.218335      0.951070\n",
      "     9     0.2790     0.2834      0.2814    208     8    0.0385 0.220050       1667       319      0.191362  1.093746      0.975535\n",
      "    10     0.2474     0.2784      0.2648    202     8    0.0396 0.226338       1869       327      0.174960  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2024Q3\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7523     0.7764      0.7584     89    68    0.7640 4.319901         89        68      0.764045  4.320155      0.419753\n",
      "     2     0.6503     0.7497      0.7195     94    51    0.5426 3.068035        183       119      0.650273  3.676854      0.734568\n",
      "     3     0.3203     0.6459      0.5391     86    19    0.2209 1.249040        269       138      0.513011  2.900730      0.851852\n",
      "     4     0.3003     0.3156      0.3102     86     1    0.0116 0.065590        355       139      0.391549  2.213945      0.858025\n",
      "     5     0.2987     0.3001      0.2992     42     2    0.0476 0.269146        397       141      0.355164  2.008210      0.870370\n",
      "     6     0.2918     0.2985      0.2964    143     4    0.0280 0.158321        540       145      0.268519  1.518290      0.895062\n",
      "     7     0.2910     0.2910      0.2910     15     1    0.0667 0.377143        555       146      0.263063  1.487443      0.901235\n",
      "     8     0.2837     0.2889      0.2878    156     8    0.0513 0.290067        711       154      0.216596  1.224705      0.950617\n",
      "     9     0.2790     0.2834      0.2812    111     5    0.0450 0.254444        822       159      0.193431  1.093719      0.981481\n",
      "    10     0.2474     0.2784      0.2619     94     3    0.0319 0.180373        916       162      0.176856  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2024Q4\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7497     0.7817      0.7564     93    66    0.7097 4.352200         93        66      0.709677  4.352062      0.437086\n",
      "     2     0.6425     0.7497      0.6955     92    41    0.4457 2.733233        185       107      0.578378  3.546877      0.708609\n",
      "     3     0.3136     0.6421      0.4353     93    15    0.1613 0.989164        278       122      0.438849  2.691219      0.807947\n",
      "     4     0.2987     0.3136      0.3054     92     4    0.0435 0.266762        370       126      0.340541  2.088348      0.834437\n",
      "     5     0.2985     0.2987      0.2985     93     7    0.0753 0.461774        463       133      0.287257  1.761589      0.880795\n",
      "     6     0.2889     0.2985      0.2940     92     2    0.0217 0.133074        555       135      0.243243  1.491677      0.894040\n",
      "     7     0.2889     0.2889      0.2889     93     6    0.0645 0.395543        648       141      0.217593  1.334376      0.933775\n",
      "     8     0.2837     0.2889      0.2867     92     4    0.0435 0.266762        740       145      0.195946  1.201629      0.960265\n",
      "     9     0.2790     0.2837      0.2814     93     5    0.0538 0.329926        833       150      0.180072  1.104283      0.993377\n",
      "    10     0.2474     0.2790      0.2637     93     1    0.0108 0.066230        926       151      0.163067  1.000000      1.000000\n",
      "\n",
      "============================================================\n",
      "Lift Table: OOT_2025Q1\n",
      "============================================================\n",
      "decile  Score_Min  Score_Max  Score_Mean  Count  Bads  Bad_Rate     Lift  Cum_Count  Cum_Bads  Cum_Bad_Rate  Cum_Lift  Capture_Rate\n",
      "     1     0.7556     0.7792      0.7602     32    27    0.8438 4.158729         32        27      0.843750  4.158482      0.385714\n",
      "     2     0.6471     0.7523      0.7247     36    26    0.7222 3.559414         68        53      0.779412  3.841387      0.757143\n",
      "     3     0.3136     0.6425      0.4173     30     4    0.1333 0.656979         98        57      0.581633  2.866618      0.814286\n",
      "     4     0.2987     0.3118      0.3050     39     1    0.0256 0.126171        137        58      0.423358  2.086548      0.828571\n",
      "     5     0.2931     0.2985      0.2977     33     1    0.0303 0.149336        170        59      0.347059  1.710504      0.842857\n",
      "     6     0.2910     0.2918      0.2917     14     1    0.0714 0.351900        184        60      0.326087  1.607143      0.857143\n",
      "     7     0.2862     0.2889      0.2888     56     4    0.0714 0.351900        240        64      0.266667  1.314286      0.914286\n",
      "     8     0.2834     0.2851      0.2838     31     2    0.0645 0.317893        271        66      0.243542  1.200316      0.942857\n",
      "     9     0.2782     0.2810      0.2798     37     4    0.1081 0.532779        308        70      0.227273  1.120130      1.000000\n",
      "    10     0.2474     0.2757      0.2611     37     0    0.0000 0.000000        345        70      0.202899  1.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "for period, lt in lift_tables.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Lift Table: {period}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(lt.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Evaluation Enhanced Steps\n",
    "\n",
    "Score PSI, Bootstrap CI, Calibration, SHAP, and Validation — same as the pipeline script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score PSI (train vs OOT periods):\n",
      "Period_1   Period_2    PSI Status\n",
      "   Train OOT_2024Q3 0.0159 Stable\n",
      "   Train OOT_2024Q4 0.0182 Stable\n",
      "   Train OOT_2025Q1 0.0402 Stable\n"
     ]
    }
   ],
   "source": [
    "# Step 9a: Score PSI — stability of predicted scores between train and OOT\n",
    "score_psi_df = None\n",
    "if config.evaluation.calculate_score_psi:\n",
    "    train_probs = final_model.predict_proba(datasets.train[selected_features])[:, 1]\n",
    "    oot_scores = {}\n",
    "    for label in sorted(datasets.oot_quarters.keys()):\n",
    "        qdf = datasets.oot_quarters[label]\n",
    "        oot_scores[f\"OOT_{label}\"] = final_model.predict_proba(qdf[selected_features])[:, 1]\n",
    "    score_psi_df = compute_score_psi(train_probs, oot_scores)\n",
    "    print(\"Score PSI (train vs OOT periods):\")\n",
    "    print(score_psi_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Score PSI disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap AUC Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:48 | INFO  | Bootstrap | Train: AUC=0.8834 [0.8716, 0.8945] (1000 iterations)\n",
      "04:36:48 | INFO  | Bootstrap | Test: AUC=0.8678 [0.8424, 0.8932] (1000 iterations)\n",
      "04:36:48 | INFO  | Bootstrap | OOT_2024Q3: AUC=0.8743 [0.8395, 0.9105] (1000 iterations)\n",
      "04:36:48 | INFO  | Bootstrap | OOT_2024Q4: AUC=0.8557 [0.8162, 0.8908] (1000 iterations)\n",
      "04:36:48 | INFO  | Bootstrap | OOT_2025Q1: AUC=0.8683 [0.8075, 0.9195] (1000 iterations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap AUC Confidence Intervals:\n",
      "    Period    AUC  CI_Lower  CI_Upper  N_Bootstrap\n",
      "     Train 0.8834    0.8716    0.8945         1000\n",
      "      Test 0.8678    0.8424    0.8932         1000\n",
      "OOT_2024Q3 0.8743    0.8395    0.9105         1000\n",
      "OOT_2024Q4 0.8557    0.8162    0.8908         1000\n",
      "OOT_2025Q1 0.8683    0.8075    0.9195         1000\n"
     ]
    }
   ],
   "source": [
    "# Step 9b: Bootstrap CI — confidence intervals on AUC for each period\n",
    "bootstrap_df = None\n",
    "if config.evaluation.bootstrap.enabled:\n",
    "    periods_for_bootstrap = [(\"Train\", datasets.train), (\"Test\", datasets.test)]\n",
    "    for label in sorted(datasets.oot_quarters.keys()):\n",
    "        periods_for_bootstrap.append((f\"OOT_{label}\", datasets.oot_quarters[label]))\n",
    "\n",
    "    bootstrap_df = bootstrap_auc_ci(\n",
    "        model=final_model,\n",
    "        selected_features=selected_features,\n",
    "        datasets=periods_for_bootstrap,\n",
    "        target_column=config.data.target_column,\n",
    "        n_iterations=config.evaluation.bootstrap.n_iterations,\n",
    "        confidence_level=config.evaluation.bootstrap.confidence_level,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "\n",
    "    # Merge CI columns into performance_df\n",
    "    if bootstrap_df is not None and not bootstrap_df.empty:\n",
    "        ci_cols = bootstrap_df[[\"Period\", \"CI_Lower\", \"CI_Upper\"]].copy()\n",
    "        performance_df = performance_df.merge(ci_cols, on=\"Period\", how=\"left\")\n",
    "\n",
    "    print(\"Bootstrap AUC Confidence Intervals:\")\n",
    "    print(bootstrap_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Bootstrap CI disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration (platt):\n",
      "  Brier score: 0.1472 -> 0.0897\n",
      "  ECE:         0.2335 -> 0.0332\n",
      "  H-L chi2:    17.5369 (p=0.0142)\n"
     ]
    }
   ],
   "source": [
    "# Step 9c: Calibration — fit on test set, report Brier/ECE improvement\n",
    "calibration_dict = None\n",
    "if config.evaluation.calibration.enabled:\n",
    "    from src.evaluation.calibrator import ModelCalibrator\n",
    "    calibrator = ModelCalibrator(method=config.evaluation.calibration.method)\n",
    "    test_probs = final_model.predict_proba(datasets.test[selected_features])[:, 1]\n",
    "    y_test_vals = datasets.test[config.data.target_column].values\n",
    "    calibrator.fit(y_test_vals, test_probs)\n",
    "    cal_result = calibrator.get_calibration_result(y_test_vals, test_probs)\n",
    "    calibration_dict = cal_result.to_dict()\n",
    "\n",
    "    print(f\"Calibration ({cal_result.method}):\")\n",
    "    print(f\"  Brier score: {cal_result.brier_score_before:.4f} -> {cal_result.brier_score_after:.4f}\")\n",
    "    print(f\"  ECE:         {cal_result.ece_before:.4f} -> {cal_result.ece_after:.4f}\")\n",
    "    print(f\"  H-L chi2:    {cal_result.hosmer_lemeshow_chi2:.4f} (p={cal_result.hosmer_lemeshow_pvalue:.4f})\")\n",
    "else:\n",
    "    print(\"Calibration disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:54 | INFO  | Sampled 500 rows from 7473 for SHAP computation.\n",
      "04:36:54 | INFO  | Computed SHAP values: 500 samples x 3 features.\n",
      "04:36:54 | INFO  | Saved SHAP bar plot: outputs/model_development/20260212_043612_1dc021/reports/shap_summary_bar.png\n",
      "04:36:54 | INFO  | Saved SHAP beeswarm plot: outputs/model_development/20260212_043612_1dc021/reports/shap_beeswarm.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP Summary:\n",
      "                 Feature  Mean_Abs_SHAP  Rank\n",
      "      default_count_ever       0.908176     1\n",
      "  recovered_total_amount       0.048540     2\n",
      "newest_credit_age_months       0.041280     3\n",
      "\n",
      "SHAP plots saved: ['outputs/model_development/20260212_043612_1dc021/reports/shap_summary_bar.png', 'outputs/model_development/20260212_043612_1dc021/reports/shap_beeswarm.png']\n"
     ]
    }
   ],
   "source": [
    "# Step 9d: SHAP — TreeExplainer values + summary plots\n",
    "shap_summary = None\n",
    "shap_plot_paths = None\n",
    "if config.evaluation.shap.enabled:\n",
    "    from src.model_development.shap_analyzer import (\n",
    "        compute_shap_values,\n",
    "        shap_summary_df,\n",
    "        save_shap_plots,\n",
    "    )\n",
    "    shap_vals, feat_names, X_shap = compute_shap_values(\n",
    "        model=final_model,\n",
    "        X=datasets.train[selected_features],\n",
    "        max_samples=config.evaluation.shap.max_samples,\n",
    "    )\n",
    "    shap_summary = shap_summary_df(shap_vals, feat_names)\n",
    "\n",
    "    # Save plots to run directory\n",
    "    shap_output_dir = str(output_manager.run_dir / \"reports\")\n",
    "    shap_plot_paths = save_shap_plots(shap_vals, X_shap, shap_output_dir)\n",
    "\n",
    "    print(\"SHAP Summary:\")\n",
    "    print(shap_summary.to_string(index=False))\n",
    "    if shap_plot_paths:\n",
    "        print(f\"\\nSHAP plots saved: {shap_plot_paths}\")\n",
    "else:\n",
    "    print(\"SHAP analysis disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Checks\n",
    "Automated quality checks: discrimination, overfitting, OOT stability, score PSI, concentration, monotonicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:55 | INFO  | MODEL_CHECK | PASS | Discrimination (AUC) | All periods have AUC >= 0.65 (min observed: 0.8557).\n",
      "04:36:55 | INFO  | MODEL_CHECK | PASS | Discrimination (Gini) | All Gini values >= 0.3000 (min: 0.7115).\n",
      "04:36:55 | INFO  | MODEL_CHECK | PASS | Overfitting | AUC gap Train-Test = 0.0156 within threshold 0.0500.\n",
      "04:36:55 | INFO  | MODEL_CHECK | PASS | OOT stability | All OOT periods within 0.0800 of test AUC (min OOT AUC: 0.8557).\n",
      "04:36:55 | INFO  | MODEL_CHECK | PASS | Score PSI | Score PSI 0.0402 within threshold 0.2500.\n",
      "04:36:55 | WARNING | MODEL_CHECK | WARNING | Feature concentration | Feature 'default_count_ever' contributes 94.41% of total importance (threshold: 50%).\n",
      "04:36:55 | INFO  | MODEL_CHECK | PASS | OOT sample size | All OOT periods have >= 30 bads (min: 70).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Report: 6 PASS, 1 WARNING, 0 FAIL\n",
      "  [+] Discrimination (AUC): All periods have AUC >= 0.65 (min observed: 0.8557).\n",
      "  [+] Discrimination (Gini): All Gini values >= 0.3000 (min: 0.7115).\n",
      "  [+] Overfitting: AUC gap Train-Test = 0.0156 within threshold 0.0500.\n",
      "  [+] OOT stability: All OOT periods within 0.0800 of test AUC (min OOT AUC: 0.8557).\n",
      "  [+] Score PSI: Score PSI 0.0402 within threshold 0.2500.\n",
      "  [!] Feature concentration: Feature 'default_count_ever' contributes 94.41% of total importance (threshold: 50%).\n",
      "  [+] OOT sample size: All OOT periods have >= 30 bads (min: 70).\n",
      "\n",
      "Recommendations:\n",
      "  [WARNING] Feature concentration: Consider removing dominant feature and retraining for a more robust model.\n"
     ]
    }
   ],
   "source": [
    "# Step 9e: Validation — same checks as the pipeline script\n",
    "validation_report_df = None\n",
    "has_critical_failures = False\n",
    "if config.validation.enabled:\n",
    "    model_validator = ModelValidator(config)\n",
    "    val_report = model_validator.validate(\n",
    "        performance_df=performance_df,\n",
    "        importance_df=importance_df,\n",
    "        score_psi_df=score_psi_df,\n",
    "    )\n",
    "    validation_report_df = val_report.to_dataframe()\n",
    "    has_critical_failures = val_report.has_critical_failures\n",
    "\n",
    "    print(val_report.summary())\n",
    "\n",
    "    # Recommendations for failures/warnings\n",
    "    issues = [c for c in val_report.checks if c.status.value != \"PASS\"]\n",
    "    if issues:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for check in issues:\n",
    "            if check.recommendation:\n",
    "                print(f\"  [{check.status.value}] {check.check_name}: {check.recommendation}\")\n",
    "else:\n",
    "    print(\"Validation checks disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Outputs & Generate Excel Report\n",
    "\n",
    "Produces the same Excel report with all sheets as the script pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:57 | INFO  | EXCEL | Embedded selection chart in 06_Selection\n",
      "04:36:57 | INFO  | EXCEL | Embedded SHAP plot in 10_SHAP\n",
      "04:36:57 | INFO  | COMPLETE | Excel saved: outputs/model_development/20260212_043612_1dc021/reports/model_dev_20260212_043612_1dc021.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: outputs/model_development/20260212_043612_1dc021/data/model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:57 | INFO  | Run metadata saved to outputs/model_development/20260212_043612_1dc021/run_metadata.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run ID: 20260212_043612_1dc021\n",
      "Excel report: outputs/model_development/20260212_043612_1dc021/reports/model_dev_20260212_043612_1dc021.xlsx\n",
      "Selection chart: outputs/model_development/20260212_043612_1dc021/reports/selection_chart_20260212_043626.png\n",
      "SHAP plots: ['outputs/model_development/20260212_043612_1dc021/reports/shap_summary_bar.png', 'outputs/model_development/20260212_043612_1dc021/reports/shap_beeswarm.png']\n",
      "All outputs: outputs/model_development/20260212_043612_1dc021\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Build summary dict (same structure as pipeline._build_summary)\n",
    "n_total = len(datasets.feature_columns)\n",
    "n_after_const = const_result.n_kept\n",
    "n_after_missing = missing_result.n_kept\n",
    "n_after_iv = iv_result.n_kept\n",
    "n_after_psi = psi_result.n_kept\n",
    "n_after_corr = corr_result.n_kept\n",
    "n_after_sel = len(selected_features) + (vif_result.n_eliminated if vif_result else 0)\n",
    "n_after_vif = len(selected_features)\n",
    "\n",
    "train_dates = datasets.train[config.data.date_column]\n",
    "oot_labels_str = \", \".join(datasets.oot_labels) if datasets.oot_labels else \"None\"\n",
    "\n",
    "summary = {\n",
    "    \"Run Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Run ID\": output_manager.run_id,\n",
    "    \"Input File\": config.data.input_path,\n",
    "    \"Train End Date\": config.splitting.train_end_date,\n",
    "    \"Train Period\": f\"{train_dates.min().strftime('%Y-%m-%d')} to {train_dates.max().strftime('%Y-%m-%d')}\",\n",
    "    \"OOT Periods\": oot_labels_str,\n",
    "    \"Train Rows\": len(datasets.train),\n",
    "    \"Test Rows\": len(datasets.test),\n",
    "    \"Train Bad Rate\": f\"{datasets.train[target].mean():.2%}\",\n",
    "    \"Test Bad Rate\": f\"{datasets.test[target].mean():.2%}\",\n",
    "    \"\": \"\",\n",
    "    \"Total Features\": n_total,\n",
    "    \"After Constant Elimination\": f\"{n_after_const} ({n_total - n_after_const} eliminated)\",\n",
    "    \"After Missing Elimination\": f\"{n_after_missing} ({n_after_const - n_after_missing} eliminated)\",\n",
    "    \"After IV Elimination\": f\"{n_after_iv} ({n_after_missing - n_after_iv} eliminated)\",\n",
    "    \"After PSI Elimination\": f\"{n_after_psi} ({n_after_iv - n_after_psi} eliminated)\",\n",
    "    \"After Correlation Elimination\": f\"{n_after_corr} ({n_after_psi - n_after_corr} eliminated)\",\n",
    "    \"After Sequential Selection\": f\"{n_after_sel} ({n_after_corr - n_after_sel} skipped)\",\n",
    "    \"After VIF Check\": f\"{n_after_vif} ({n_after_sel - n_after_vif} eliminated)\",\n",
    "    \" \": \"\",\n",
    "    \"Selection Method\": config.steps.selection.method,\n",
    "    \"Selection CV Folds\": config.steps.selection.cv,\n",
    "}\n",
    "\n",
    "if config.model.tuning.enabled and best_params:\n",
    "    summary[\"Tuning Enabled\"] = \"Yes\"\n",
    "    summary[\"Tuning Trials\"] = config.model.tuning.n_trials\n",
    "    best_auc = best_params.get(\"_best_cv_auc\", \"N/A\")\n",
    "    summary[\"Tuning Best CV AUC\"] = best_auc\n",
    "else:\n",
    "    summary[\"Tuning Enabled\"] = \"No\"\n",
    "\n",
    "summary[\"  \"] = \"\"\n",
    "for _, row in performance_df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    summary[f\"AUC {period}\"] = row[\"AUC\"]\n",
    "    summary[f\"Gini {period}\"] = row[\"Gini\"]\n",
    "\n",
    "summary[\"   \"] = \"\"\n",
    "summary[\"IV Range\"] = f\"[{config.steps.iv.min_iv}, {config.steps.iv.max_iv}]\"\n",
    "summary[\"Missing Threshold\"] = f\"{config.steps.missing.threshold:.0%}\"\n",
    "summary[\"PSI Threshold\"] = str(config.steps.psi.threshold)\n",
    "summary[\"Correlation Threshold\"] = str(config.steps.correlation.threshold)\n",
    "summary[\"VIF Threshold\"] = str(config.steps.vif.threshold) if config.steps.vif.enabled else \"Disabled\"\n",
    "\n",
    "# Generate Excel report (same call as pipeline.py)\n",
    "excel_path = str(output_manager.run_dir / \"reports\" / f\"model_dev_{output_manager.run_id}.xlsx\")\n",
    "\n",
    "excel_reporter.generate_report(\n",
    "    output_path=excel_path,\n",
    "    summary=summary,\n",
    "    elimination_results=elimination_results,\n",
    "    corr_pairs_df=corr_pairs_df,\n",
    "    selection_df=selection_df,\n",
    "    performance_df=performance_df,\n",
    "    lift_tables=lift_tables,\n",
    "    importance_df=importance_df,\n",
    "    vif_df=vif_result.details_df if vif_result else None,\n",
    "    tuning_df=tuning_df,\n",
    "    tuning_best_params=best_params,\n",
    "    chart_path=chart_path,\n",
    "    score_psi_df=score_psi_df,\n",
    "    bootstrap_df=bootstrap_df,\n",
    "    shap_summary_df=shap_summary,\n",
    "    shap_plot_path=shap_plot_paths[0] if shap_plot_paths else None,\n",
    "    calibration_dict=calibration_dict,\n",
    "    validation_report_df=validation_report_df,\n",
    ")\n",
    "\n",
    "# Step 9f: Save model artifact\n",
    "model_path = None\n",
    "if config.output.save_model:\n",
    "    model_path = output_manager.save_artifact(\"model\", final_model, fmt=\"joblib\")\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save run metadata\n",
    "output_manager.mark_complete(\"success\")\n",
    "if config.reproducibility.save_metadata:\n",
    "    output_manager.save_run_metadata()\n",
    "\n",
    "print(f\"\\nRun ID: {output_manager.run_id}\")\n",
    "print(f\"Excel report: {excel_path}\")\n",
    "if chart_path:\n",
    "    print(f\"Selection chart: {chart_path}\")\n",
    "if shap_plot_paths:\n",
    "    print(f\"SHAP plots: {shap_plot_paths}\")\n",
    "if has_critical_failures:\n",
    "    print(\"WARNING: Validation found critical failures!\")\n",
    "print(f\"All outputs: {output_manager.run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative: Full Pipeline Mode\n",
    "\n",
    "Run everything in one call using `ModelDevelopmentPipeline` — the exact same class used by the CLI script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the full pipeline in one call:\n",
    "\n",
    "# from src.config.loader import load_config, save_config\n",
    "# from src.io.output_manager import OutputManager\n",
    "# from src.model_development.pipeline import ModelDevelopmentPipeline\n",
    "# from src.model_development.eliminators import (\n",
    "#     QuarterlyPSICheck, YearlyPSICheck, ConsecutiveQuarterPSICheck,\n",
    "#     HalfSplitPSICheck, DateSplitPSICheck,\n",
    "# )\n",
    "#\n",
    "# config = load_config(\"config/model_development.yaml\")\n",
    "# output_manager = OutputManager(config)\n",
    "#\n",
    "# if config.reproducibility.save_config:\n",
    "#     save_config(config, str(output_manager.run_dir / \"config\" / \"pipeline_config.yaml\"))\n",
    "#\n",
    "# # Build PSI checks from config\n",
    "# check_map = {\"quarterly\": QuarterlyPSICheck, \"yearly\": YearlyPSICheck,\n",
    "#               \"consecutive\": ConsecutiveQuarterPSICheck, \"halfsplit\": HalfSplitPSICheck}\n",
    "# psi_checks = []\n",
    "# for c in config.steps.psi.checks:\n",
    "#     if c.type in check_map:\n",
    "#         psi_checks.append(check_map[c.type]())\n",
    "#     elif c.type == \"date_split\" and c.date:\n",
    "#         psi_checks.append(DateSplitPSICheck(c.date, label=c.label))\n",
    "#\n",
    "# pipeline = ModelDevelopmentPipeline(\n",
    "#     input_path=config.data.input_path,\n",
    "#     train_end_date=config.splitting.train_end_date,\n",
    "#     output_dir=str(output_manager.run_dir / \"reports\"),\n",
    "#     iv_min=config.steps.iv.min_iv,\n",
    "#     iv_max=config.steps.iv.max_iv,\n",
    "#     missing_threshold=config.steps.missing.threshold,\n",
    "#     psi_threshold=config.steps.psi.threshold,\n",
    "#     correlation_threshold=config.steps.correlation.threshold,\n",
    "#     test_size=config.splitting.test_size,\n",
    "#     target_column=config.data.target_column,\n",
    "#     date_column=config.data.date_column,\n",
    "#     xgb_params=config.model.params,\n",
    "#     psi_checks=psi_checks,\n",
    "#     selection_method=config.steps.selection.method,\n",
    "#     selection_cv=config.steps.selection.cv,\n",
    "#     selection_max_features=config.steps.selection.max_features,\n",
    "#     selection_min_features=config.steps.selection.min_features,\n",
    "#     selection_tolerance=config.steps.selection.tolerance,\n",
    "#     selection_patience=config.steps.selection.patience,\n",
    "#     vif_enabled=config.steps.vif.enabled,\n",
    "#     vif_threshold=config.steps.vif.threshold,\n",
    "#     vif_iv_aware=config.steps.vif.iv_aware,\n",
    "#     tuning_enabled=config.model.tuning.enabled,\n",
    "#     tuning_n_trials=config.model.tuning.n_trials,\n",
    "#     tuning_timeout=config.model.tuning.timeout,\n",
    "#     tuning_cv=config.model.tuning.cv,\n",
    "#     config=config,\n",
    "#     output_manager=output_manager,\n",
    "# )\n",
    "#\n",
    "# results = pipeline.run()\n",
    "#\n",
    "# output_manager.mark_complete(results.get(\"status\", \"unknown\"))\n",
    "# if config.reproducibility.save_metadata:\n",
    "#     output_manager.save_run_metadata()\n",
    "#\n",
    "# print(f\"Status: {results['status']}\")\n",
    "# print(f\"Selected features: {results['after_selection']}\")\n",
    "# print(f\"Excel: {results['excel_path']}\")\n",
    "# print(f\"Run dir: {output_manager.run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
