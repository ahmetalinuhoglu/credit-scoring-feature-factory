{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Credit Scoring Model Development\n",
    "\n",
    "Interactive notebook for step-by-step development of a classic credit risk model.\n",
    "Uses **Weight of Evidence (WoE)** encoding, **Logistic Regression**, and a **point-based Scorecard**.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup and imports\n",
    "2. Load config\n",
    "3. Load and split data\n",
    "4. WoE binning fit\n",
    "5. IV summary and visualization\n",
    "6. WoE visualization for top features\n",
    "7. IV-based feature selection\n",
    "8. PSI check on WoE values\n",
    "9. Correlation on WoE features\n",
    "10. Logistic Regression fit\n",
    "11. Model diagnostics\n",
    "12. Scorecard generation\n",
    "13. Scorecard application\n",
    "14. Evaluation metrics\n",
    "15. Lift tables\n",
    "16. Bootstrap CI and Score PSI\n",
    "17. Save outputs\n",
    "18. Alternative: full pipeline mode\n",
    "\n",
    "Each cell is self-contained and re-runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "project_root = str(Path.cwd().parent) if Path.cwd().name == \"notebooks\" else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-5s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "from src.model_development.data_loader import load_and_split\n",
    "from src.features.woe_transformer import WoETransformer\n",
    "from src.classic_model.model_adapter import ClassicModelAdapter\n",
    "from src.classic_model.scorecard import ScorecardGenerator\n",
    "from src.model_development.evaluator import (\n",
    "    evaluate_model_quarterly,\n",
    "    bootstrap_auc_ci,\n",
    "    compute_score_psi,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working dir:  {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from src.config.schema import ClassicPipelineConfig\n",
    "\n",
    "config_path = \"config/classic_model.yaml\"\n",
    "with open(config_path) as f:\n",
    "    raw_config = yaml.safe_load(f)\n",
    "\n",
    "config = ClassicPipelineConfig(**raw_config)\n",
    "\n",
    "seed = config.reproducibility.global_seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "n_jobs = config.reproducibility.n_jobs\n",
    "\n",
    "print(f\"Input:          {config.data.input_path}\")\n",
    "print(f\"Train end date: {config.splitting.train_end_date}\")\n",
    "print(f\"Target:         {config.data.target_column}\")\n",
    "print(f\"Seed:           {seed}\")\n",
    "print(f\"n_jobs:         {n_jobs}\")\n",
    "print(f\"WoE bins:       {config.woe.n_bins}\")\n",
    "print(f\"WoE monotonic:  {config.woe.monotonic}\")\n",
    "print(f\"IV range:       [{config.woe.min_iv}, {config.woe.max_iv}]\")\n",
    "print(f\"LogReg solver:  {config.logistic.solver}\")\n",
    "print(f\"LogReg C:       {config.logistic.C}\")\n",
    "print(f\"Scorecard:      score={config.scorecard.target_score}, odds={config.scorecard.target_odds}, pdo={config.scorecard.pdo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_and_split(\n",
    "    input_path=config.data.input_path,\n",
    "    train_end_date=config.splitting.train_end_date,\n",
    "    target_column=config.data.target_column,\n",
    "    date_column=config.data.date_column,\n",
    "    id_columns=list(config.data.id_columns),\n",
    "    meta_columns=list(config.data.exclude_columns),\n",
    "    test_size=config.splitting.test_size,\n",
    "    stratify=config.splitting.stratify,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "target = config.data.target_column\n",
    "features = list(datasets.feature_columns)\n",
    "\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Train: {len(datasets.train):,} rows, bad rate: {datasets.train[target].mean():.2%}\")\n",
    "print(f\"Test:  {len(datasets.test):,} rows, bad rate: {datasets.test[target].mean():.2%}\")\n",
    "for label in datasets.oot_labels:\n",
    "    qdf = datasets.oot_quarters[label]\n",
    "    print(f\"OOT {label}: {len(qdf):,} rows, bad rate: {qdf[target].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WoE Binning Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_config = {\n",
    "    \"model\": {\n",
    "        \"logistic_regression\": {\n",
    "            \"woe_binning\": {\n",
    "                \"n_bins\": config.woe.n_bins,\n",
    "                \"min_bin_size\": config.woe.min_bin_size,\n",
    "                \"monotonic\": config.woe.monotonic,\n",
    "                \"missing_bin\": config.woe.missing_bin,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "woe_transformer = WoETransformer(config=woe_config, name=\"ClassicWoE\")\n",
    "woe_transformer.fit(datasets.train, features, target_column=target)\n",
    "\n",
    "print(f\"Fitted WoE for {len(woe_transformer.fitted_features)} features\")\n",
    "\n",
    "# Transform all sets\n",
    "woe_train = woe_transformer.transform(datasets.train)\n",
    "woe_test = woe_transformer.transform(datasets.test)\n",
    "woe_oot = {}\n",
    "for label, qdf in datasets.oot_quarters.items():\n",
    "    woe_oot[label] = woe_transformer.transform(qdf)\n",
    "\n",
    "print(f\"WoE columns added: {sum(1 for c in woe_train.columns if c.endswith('_woe'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. IV Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_summary = woe_transformer.get_iv_summary()\n",
    "iv_df = pd.DataFrame([\n",
    "    {\"Feature\": f, \"IV\": iv, \"Category\": woe_transformer.get_iv_category(iv)}\n",
    "    for f, iv in sorted(iv_summary.items(), key=lambda x: -x[1])\n",
    "])\n",
    "\n",
    "print(f\"Total features with IV: {len(iv_df)}\")\n",
    "print(f\"\\nIV Category Distribution:\")\n",
    "print(iv_df[\"Category\"].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nTop 20 features by IV:\")\n",
    "print(iv_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV distribution histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of all IV values\n",
    "axes[0].hist(iv_df[\"IV\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=config.woe.min_iv, color=\"red\", linestyle=\"--\", label=f\"min_iv={config.woe.min_iv}\")\n",
    "axes[0].axvline(x=config.woe.max_iv, color=\"orange\", linestyle=\"--\", label=f\"max_iv={config.woe.max_iv}\")\n",
    "axes[0].set_xlabel(\"Information Value\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"IV Distribution (All Features)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Top 20 features bar chart\n",
    "top20 = iv_df.head(20)\n",
    "colors = []\n",
    "for iv in top20[\"IV\"]:\n",
    "    if iv > config.woe.max_iv:\n",
    "        colors.append(\"red\")\n",
    "    elif iv >= 0.3:\n",
    "        colors.append(\"#2ecc71\")\n",
    "    elif iv >= 0.1:\n",
    "        colors.append(\"#3498db\")\n",
    "    else:\n",
    "        colors.append(\"#95a5a6\")\n",
    "\n",
    "axes[1].barh(range(len(top20)), top20[\"IV\"].values, color=colors)\n",
    "axes[1].set_yticks(range(len(top20)))\n",
    "axes[1].set_yticklabels(top20[\"Feature\"].values, fontsize=8)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel(\"Information Value\")\n",
    "axes[1].set_title(\"Top 20 Features by IV\")\n",
    "axes[1].axvline(x=config.woe.min_iv, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].axvline(x=config.woe.max_iv, color=\"orange\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. WoE Visualization for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize WoE bins for top 4 features by IV\n",
    "top_features = iv_df[\n",
    "    (iv_df[\"IV\"] >= config.woe.min_iv) & (iv_df[\"IV\"] <= config.woe.max_iv)\n",
    "][\"Feature\"].tolist()[:4]\n",
    "\n",
    "if top_features:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feat in enumerate(top_features):\n",
    "        ax = axes[idx]\n",
    "        bins = woe_transformer.get_woe_table(feat)\n",
    "        if bins is None:\n",
    "            continue\n",
    "\n",
    "        regular_bins = sorted(\n",
    "            [b for b in bins if b.bin_id >= 0],\n",
    "            key=lambda b: b.lower_bound,\n",
    "        )\n",
    "        if not regular_bins:\n",
    "            continue\n",
    "\n",
    "        labels = [f\"[{b.lower_bound:.2g}, {b.upper_bound:.2g}]\" for b in regular_bins]\n",
    "        woe_vals = [b.woe for b in regular_bins]\n",
    "        bad_rates = [b.bad_count / b.count if b.count > 0 else 0 for b in regular_bins]\n",
    "\n",
    "        color = [\"#e74c3c\" if w < 0 else \"#2ecc71\" for w in woe_vals]\n",
    "        ax.bar(range(len(woe_vals)), woe_vals, color=color, alpha=0.8)\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=45, fontsize=7, ha=\"right\")\n",
    "        ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "        ax.set_ylabel(\"WoE\")\n",
    "\n",
    "        iv_val = iv_summary.get(feat, 0)\n",
    "        ax.set_title(f\"{feat}\\nIV={iv_val:.4f}\", fontsize=10)\n",
    "\n",
    "        # Add bad rate annotation\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(range(len(bad_rates)), bad_rates, \"ko-\", markersize=4, alpha=0.5)\n",
    "        ax2.set_ylabel(\"Bad Rate\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No features in the valid IV range to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. IV-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "for feat, iv in sorted(iv_summary.items(), key=lambda x: -x[1]):\n",
    "    if config.woe.min_iv <= iv <= config.woe.max_iv:\n",
    "        selected_features.append(feat)\n",
    "\n",
    "woe_selected = [f\"{f}_woe\" for f in selected_features]\n",
    "\n",
    "print(f\"IV selection: {len(iv_summary)} -> {len(selected_features)} features\")\n",
    "print(f\"  min_iv={config.woe.min_iv}, max_iv={config.woe.max_iv}\")\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features[:20], 1):\n",
    "    print(f\"  {i}. {feat} (IV={iv_summary[feat]:.4f})\")\n",
    "if len(selected_features) > 20:\n",
    "    print(f\"  ... and {len(selected_features) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PSI Check on WoE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = config.data.date_column\n",
    "train_dates = woe_train[date_col]\n",
    "median_date = train_dates.median()\n",
    "mask_first = train_dates <= median_date\n",
    "mask_second = train_dates > median_date\n",
    "\n",
    "first_half = woe_train.loc[mask_first]\n",
    "second_half = woe_train.loc[mask_second]\n",
    "\n",
    "print(f\"PSI split: first_half={len(first_half):,}, second_half={len(second_half):,}\")\n",
    "\n",
    "psi_rows = []\n",
    "psi_threshold = 0.25\n",
    "\n",
    "for feat in woe_selected:\n",
    "    try:\n",
    "        exp = first_half[feat].dropna().values\n",
    "        act = second_half[feat].dropna().values\n",
    "        if len(exp) < 10 or len(act) < 10:\n",
    "            psi_val = None\n",
    "        else:\n",
    "            _, bins = pd.qcut(exp, q=10, retbins=True, duplicates=\"drop\")\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            exp_pct = pd.cut(exp, bins=bins).value_counts(normalize=True).sort_index().clip(lower=1e-4)\n",
    "            act_pct = pd.cut(act, bins=bins).value_counts(normalize=True).sort_index().clip(lower=1e-4)\n",
    "            all_bins = exp_pct.index.union(act_pct.index)\n",
    "            exp_pct = exp_pct.reindex(all_bins, fill_value=1e-4)\n",
    "            act_pct = act_pct.reindex(all_bins, fill_value=1e-4)\n",
    "            psi_val = float(((act_pct - exp_pct) * np.log(act_pct / exp_pct)).sum())\n",
    "            if not np.isfinite(psi_val):\n",
    "                psi_val = None\n",
    "    except Exception:\n",
    "        psi_val = None\n",
    "\n",
    "    status = \"N/A\" if psi_val is None else (\"Stable\" if psi_val < 0.10 else (\"Moderate\" if psi_val < 0.25 else \"Unstable\"))\n",
    "    psi_rows.append({\"Feature\": feat, \"PSI\": round(psi_val, 4) if psi_val else None, \"Status\": status})\n",
    "\n",
    "psi_df = pd.DataFrame(psi_rows)\n",
    "unstable = psi_df[psi_df[\"Status\"] == \"Unstable\"]\n",
    "\n",
    "if len(unstable) > 0:\n",
    "    print(f\"\\nUnstable features ({len(unstable)}):\")\n",
    "    print(unstable.to_string(index=False))\n",
    "    # Drop unstable\n",
    "    woe_selected = [f for f in woe_selected if f not in unstable[\"Feature\"].tolist()]\n",
    "    selected_features = [c.replace(\"_woe\", \"\") for c in woe_selected]\n",
    "    print(f\"\\nAfter PSI filter: {len(woe_selected)} WoE features\")\n",
    "else:\n",
    "    print(f\"\\nAll {len(woe_selected)} features are stable (PSI < {psi_threshold})\")\n",
    "\n",
    "print(f\"\\nPSI summary:\")\n",
    "print(psi_df[\"Status\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Correlation on WoE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_threshold = 0.80\n",
    "\n",
    "if len(woe_selected) < 2:\n",
    "    print(\"Not enough features for correlation check\")\n",
    "    corr_pairs_df = pd.DataFrame()\n",
    "else:\n",
    "    corr_matrix = woe_train[woe_selected].corr(method=\"pearson\").abs()\n",
    "\n",
    "    to_drop = set()\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(woe_selected)):\n",
    "        for j in range(i + 1, len(woe_selected)):\n",
    "            feat_a = woe_selected[i]\n",
    "            feat_b = woe_selected[j]\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "\n",
    "            if corr_val >= corr_threshold:\n",
    "                orig_a = feat_a.replace(\"_woe\", \"\")\n",
    "                orig_b = feat_b.replace(\"_woe\", \"\")\n",
    "                iv_a = iv_summary.get(orig_a, 0)\n",
    "                iv_b = iv_summary.get(orig_b, 0)\n",
    "\n",
    "                if iv_a >= iv_b:\n",
    "                    to_drop.add(feat_b)\n",
    "                    drop, keep = feat_b, feat_a\n",
    "                else:\n",
    "                    to_drop.add(feat_a)\n",
    "                    drop, keep = feat_a, feat_b\n",
    "\n",
    "                pairs.append({\n",
    "                    \"Feature_A\": feat_a, \"Feature_B\": feat_b,\n",
    "                    \"Correlation\": round(corr_val, 4),\n",
    "                    \"Dropped\": drop, \"Kept\": keep,\n",
    "                })\n",
    "\n",
    "    corr_pairs_df = pd.DataFrame(pairs)\n",
    "    woe_selected = [f for f in woe_selected if f not in to_drop]\n",
    "    selected_features = [c.replace(\"_woe\", \"\") for c in woe_selected]\n",
    "\n",
    "    print(f\"Correlation elimination: {len(corr_matrix)} -> {len(woe_selected)} features\")\n",
    "    print(f\"  threshold={corr_threshold}, {len(to_drop)} dropped\")\n",
    "\n",
    "    if len(corr_pairs_df) > 0:\n",
    "        print(f\"\\nCorrelated pairs ({len(corr_pairs_df)}):\")\n",
    "        print(corr_pairs_df.head(15).to_string(index=False))\n",
    "\n",
    "print(f\"\\nFinal WoE features ({len(woe_selected)}):\")\n",
    "for i, feat in enumerate(woe_selected, 1):\n",
    "    orig = feat.replace(\"_woe\", \"\")\n",
    "    print(f\"  {i}. {feat} (IV={iv_summary.get(orig, 0):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Logistic Regression Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe = woe_train[woe_selected].values\n",
    "y_train_vals = woe_train[target].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_woe)\n",
    "\n",
    "# Resolve config\n",
    "class_weight = config.logistic.class_weight\n",
    "if class_weight == \"none\" or class_weight is None:\n",
    "    class_weight = None\n",
    "\n",
    "penalty = config.logistic.penalty\n",
    "if penalty == \"none\":\n",
    "    penalty = None\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    solver=config.logistic.solver,\n",
    "    penalty=penalty,\n",
    "    C=config.logistic.C,\n",
    "    max_iter=config.logistic.max_iter,\n",
    "    class_weight=class_weight,\n",
    "    random_state=seed,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train_vals)\n",
    "\n",
    "print(\"Logistic Regression fitted.\")\n",
    "print(f\"  Solver: {config.logistic.solver}\")\n",
    "print(f\"  Penalty: {penalty}\")\n",
    "print(f\"  C: {config.logistic.C}\")\n",
    "print(f\"  Features: {len(woe_selected)}\")\n",
    "print(f\"  Intercept: {lr_model.intercept_[0]:.4f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": woe_selected,\n",
    "    \"Coefficient\": lr_model.coef_[0],\n",
    "    \"Abs_Coefficient\": np.abs(lr_model.coef_[0]),\n",
    "}).sort_values(\"Abs_Coefficient\", ascending=False)\n",
    "print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adapter for evaluation\n",
    "adapter = ClassicModelAdapter(lr_model, scaler, woe_selected)\n",
    "\n",
    "# Quick AUC check\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "train_probs = adapter.predict_proba(woe_train[woe_selected])[:, 1]\n",
    "test_probs = adapter.predict_proba(woe_test[woe_selected])[:, 1]\n",
    "\n",
    "train_auc = roc_auc_score(woe_train[target], train_probs)\n",
    "test_auc = roc_auc_score(woe_test[target], test_probs)\n",
    "\n",
    "print(f\"Train AUC: {train_auc:.4f}\")\n",
    "print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"Overfit gap: {train_auc - test_auc:.4f}\")\n",
    "\n",
    "# ROC curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "for label, y_true, y_score, color in [\n",
    "    (\"Train\", woe_train[target], train_probs, \"#3498db\"),\n",
    "    (\"Test\", woe_test[target], test_probs, \"#e74c3c\"),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    ax.plot(fpr, tpr, color=color, label=f\"{label} (AUC={auc:.4f})\")\n",
    "\n",
    "for label in sorted(woe_oot.keys()):\n",
    "    qdf = woe_oot[label]\n",
    "    oot_probs = adapter.predict_proba(qdf[woe_selected])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(qdf[target], oot_probs)\n",
    "    auc = roc_auc_score(qdf[target], oot_probs)\n",
    "    ax.plot(fpr, tpr, linestyle=\"--\", alpha=0.7, label=f\"OOT {label} (AUC={auc:.4f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curves - Classic Model\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scorecard Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_gen = ScorecardGenerator(\n",
    "    target_score=config.scorecard.target_score,\n",
    "    target_odds=config.scorecard.target_odds,\n",
    "    pdo=config.scorecard.pdo,\n",
    ")\n",
    "\n",
    "scorecard_df = scorecard_gen.generate(\n",
    "    model=lr_model,\n",
    "    scaler=scaler,\n",
    "    woe_transformer=woe_transformer,\n",
    "    feature_names=selected_features,\n",
    ")\n",
    "\n",
    "print(\"Scorecard:\")\n",
    "print(scorecard_df.to_string(index=False))\n",
    "\n",
    "# Points range per feature\n",
    "print(\"\\nPoints range per feature:\")\n",
    "for feat in selected_features:\n",
    "    feat_rows = scorecard_df[scorecard_df[\"Feature\"] == feat]\n",
    "    if len(feat_rows) > 0:\n",
    "        print(f\"  {feat}: [{feat_rows['Points'].min()}, {feat_rows['Points'].max()}]\")\n",
    "\n",
    "total_min = scorecard_df.groupby(\"Feature\")[\"Points\"].min().sum()\n",
    "total_max = scorecard_df.groupby(\"Feature\")[\"Points\"].max().sum()\n",
    "print(f\"\\nTotal score range: [{total_min}, {total_max}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Scorecard Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scorecard to all datasets\n",
    "train_scores = scorecard_gen.score(datasets.train, woe_transformer, selected_features)\n",
    "test_scores = scorecard_gen.score(datasets.test, woe_transformer, selected_features)\n",
    "\n",
    "print(f\"Train scores: min={train_scores.min()}, max={train_scores.max()}, mean={train_scores.mean():.1f}, std={train_scores.std():.1f}\")\n",
    "print(f\"Test scores:  min={test_scores.min()}, max={test_scores.max()}, mean={test_scores.mean():.1f}, std={test_scores.std():.1f}\")\n",
    "\n",
    "for label in sorted(datasets.oot_quarters.keys()):\n",
    "    qdf = datasets.oot_quarters[label]\n",
    "    oot_scores = scorecard_gen.score(qdf, woe_transformer, selected_features)\n",
    "    print(f\"OOT {label}: min={oot_scores.min()}, max={oot_scores.max()}, mean={oot_scores.mean():.1f}\")\n",
    "\n",
    "# Score distribution by target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train\n",
    "ax = axes[0]\n",
    "good_mask = datasets.train[target] == 0\n",
    "ax.hist(train_scores[good_mask], bins=30, alpha=0.6, label=\"Good\", color=\"#2ecc71\", density=True)\n",
    "ax.hist(train_scores[~good_mask], bins=30, alpha=0.6, label=\"Bad\", color=\"#e74c3c\", density=True)\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Train Score Distribution\")\n",
    "ax.legend()\n",
    "\n",
    "# Test\n",
    "ax = axes[1]\n",
    "good_mask_test = datasets.test[target] == 0\n",
    "ax.hist(test_scores[good_mask_test], bins=30, alpha=0.6, label=\"Good\", color=\"#2ecc71\", density=True)\n",
    "ax.hist(test_scores[~good_mask_test], bins=30, alpha=0.6, label=\"Bad\", color=\"#e74c3c\", density=True)\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Test Score Distribution\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df, lift_tables, _ = evaluate_model_quarterly(\n",
    "    model=adapter,\n",
    "    selected_features=woe_selected,\n",
    "    train_df=woe_train,\n",
    "    test_df=woe_test,\n",
    "    oot_quarters=woe_oot,\n",
    "    target_column=target,\n",
    ")\n",
    "\n",
    "print(\"Performance by period:\")\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Coefficient-based feature importance\n",
    "abs_coefs = np.abs(lr_model.coef_[0])\n",
    "total_coef = abs_coefs.sum()\n",
    "importances = abs_coefs / total_coef if total_coef > 0 else abs_coefs\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": woe_selected,\n",
    "    \"Coefficient\": lr_model.coef_[0],\n",
    "    \"Importance\": importances,\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "importance_df[\"Rank\"] = range(1, len(importance_df) + 1)\n",
    "importance_df[\"Cumulative_Importance\"] = importance_df[\"Importance\"].cumsum()\n",
    "importance_df = importance_df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFeature Importance (coefficient-based):\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Lift Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period_name, lt in lift_tables.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Lift Table: {period_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(lt.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Bootstrap CI and Score PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CI\n",
    "bootstrap_df = None\n",
    "if config.evaluation.bootstrap.enabled:\n",
    "    periods_for_bootstrap = [(\"Train\", woe_train), (\"Test\", woe_test)]\n",
    "    for label in sorted(woe_oot.keys()):\n",
    "        periods_for_bootstrap.append((f\"OOT_{label}\", woe_oot[label]))\n",
    "\n",
    "    bootstrap_df = bootstrap_auc_ci(\n",
    "        model=adapter,\n",
    "        selected_features=woe_selected,\n",
    "        datasets=periods_for_bootstrap,\n",
    "        target_column=target,\n",
    "        n_iterations=config.evaluation.bootstrap.n_iterations,\n",
    "        confidence_level=config.evaluation.bootstrap.confidence_level,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "\n",
    "    if bootstrap_df is not None and not bootstrap_df.empty:\n",
    "        ci_cols = bootstrap_df[[\"Period\", \"CI_Lower\", \"CI_Upper\"]].copy()\n",
    "        performance_df = performance_df.merge(ci_cols, on=\"Period\", how=\"left\")\n",
    "\n",
    "    print(\"Bootstrap AUC Confidence Intervals:\")\n",
    "    print(bootstrap_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Bootstrap CI disabled\")\n",
    "\n",
    "# Score PSI\n",
    "print(\"\\n\")\n",
    "if config.evaluation.calculate_score_psi:\n",
    "    train_probs_for_psi = adapter.predict_proba(woe_train[woe_selected])[:, 1]\n",
    "    oot_scores_dict = {}\n",
    "    for label in sorted(woe_oot.keys()):\n",
    "        qdf = woe_oot[label]\n",
    "        oot_scores_dict[f\"OOT_{label}\"] = adapter.predict_proba(qdf[woe_selected])[:, 1]\n",
    "\n",
    "    score_psi_df = compute_score_psi(train_probs_for_psi, oot_scores_dict)\n",
    "    print(\"Score PSI (train vs OOT periods):\")\n",
    "    print(score_psi_df.to_string(index=False))\n",
    "else:\n",
    "    score_psi_df = None\n",
    "    print(\"Score PSI disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import numpy as np\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(config.output.base_dir) / run_id\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "excel_path = str(output_dir / f\"classic_model_{run_id}.xlsx\")\n",
    "\n",
    "# Simple Excel writer\n",
    "wb = Workbook()\n",
    "header_fill = PatternFill(\"solid\", fgColor=\"4472C4\")\n",
    "header_font = Font(bold=True, color=\"FFFFFF\", size=11)\n",
    "thin_border = Border(\n",
    "    left=Side(style=\"thin\"), right=Side(style=\"thin\"),\n",
    "    top=Side(style=\"thin\"), bottom=Side(style=\"thin\"),\n",
    ")\n",
    "\n",
    "def write_df(ws, df, start_row=1):\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), start_row):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            cell.border = thin_border\n",
    "            if r_idx == start_row:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "                cell.alignment = Alignment(horizontal=\"center\")\n",
    "\n",
    "# Sheet 1: Performance\n",
    "ws = wb.active\n",
    "ws.title = \"Performance\"\n",
    "write_df(ws, performance_df)\n",
    "\n",
    "# Sheet 2: Scorecard\n",
    "ws2 = wb.create_sheet(\"Scorecard\")\n",
    "write_df(ws2, scorecard_df)\n",
    "\n",
    "# Sheet 3: Coefficients\n",
    "ws3 = wb.create_sheet(\"Coefficients\")\n",
    "write_df(ws3, importance_df)\n",
    "\n",
    "# Sheet 4: IV Summary\n",
    "ws4 = wb.create_sheet(\"IV_Summary\")\n",
    "iv_out = iv_df.copy()\n",
    "iv_out[\"Selected\"] = iv_out[\"Feature\"].isin(selected_features)\n",
    "write_df(ws4, iv_out)\n",
    "\n",
    "# Sheet 5: Lift Tables\n",
    "ws5 = wb.create_sheet(\"Lift_Tables\")\n",
    "row = 1\n",
    "for period_name, lt in lift_tables.items():\n",
    "    ws5.cell(row=row, column=1, value=period_name).font = Font(bold=True)\n",
    "    row += 1\n",
    "    write_df(ws5, lt, start_row=row)\n",
    "    row += len(lt) + 2\n",
    "\n",
    "# Sheet 6: Bootstrap CI\n",
    "if bootstrap_df is not None and not bootstrap_df.empty:\n",
    "    ws6 = wb.create_sheet(\"Bootstrap_CI\")\n",
    "    write_df(ws6, bootstrap_df)\n",
    "\n",
    "# Sheet 7: Score PSI\n",
    "if score_psi_df is not None and not score_psi_df.empty:\n",
    "    ws7 = wb.create_sheet(\"Score_PSI\")\n",
    "    write_df(ws7, score_psi_df)\n",
    "\n",
    "wb.save(excel_path)\n",
    "print(f\"Excel report saved: {excel_path}\")\n",
    "\n",
    "# Save WoE binning\n",
    "woe_path = str(output_dir / \"woe_binning.json\")\n",
    "woe_transformer.export_binning(woe_path)\n",
    "\n",
    "# Save model\n",
    "if config.output.save_model:\n",
    "    import joblib\n",
    "    model_path = str(output_dir / \"classic_model.joblib\")\n",
    "    joblib.dump({\n",
    "        \"model\": lr_model,\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_names\": woe_selected,\n",
    "        \"selected_features\": selected_features,\n",
    "    }, model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "print(f\"\\nAll outputs: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Alternative: Full Pipeline Mode\n",
    "\n",
    "Run everything in one call using `ClassicModelPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the full pipeline in one call:\n",
    "\n",
    "# from src.classic_model.pipeline import ClassicModelPipeline\n",
    "#\n",
    "# pipeline = ClassicModelPipeline(\n",
    "#     input_path=config.data.input_path,\n",
    "#     train_end_date=config.splitting.train_end_date,\n",
    "#     output_dir=config.output.base_dir,\n",
    "#     config=config,\n",
    "# )\n",
    "#\n",
    "# results = pipeline.run()\n",
    "#\n",
    "# print(f\"Status: {results['status']}\")\n",
    "# print(f\"Selected features: {results.get('n_selected', 0)}\")\n",
    "# print(f\"Excel: {results.get('excel_path')}\")\n",
    "# print(f\"Run dir: {results.get('run_dir')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
