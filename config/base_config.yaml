# Base Configuration for Credit Scoring Pipeline
# This file contains common settings shared across all environments

project:
  name: credit-scoring-pipeline
  version: "1.0.0"
  description: "ML pipeline for credit default prediction using XGBoost and Logistic Regression"

# GCP Settings
gcp:
  project_id: ${GCP_PROJECT_ID}
  region: europe-west1
  
  bigquery:
    dataset: credit_data
    
  storage:
    bucket: ${GCS_BUCKET}
    artifacts_prefix: model_artifacts
    
  dataproc:
    cluster_name: credit-scoring-cluster
    master_machine_type: n1-standard-4
    worker_machine_type: n1-standard-4
    num_workers: 2

# Spark Settings
spark:
  app_name: CreditScoringPipeline
  master: yarn  # yarn for Dataproc, local[*] for local
  
  config:
    spark.sql.adaptive.enabled: true
    spark.sql.adaptive.coalescePartitions.enabled: true
    spark.dynamicAllocation.enabled: true
    spark.executor.memory: 4g
    spark.driver.memory: 4g
    spark.sql.shuffle.partitions: 200
    
  # BigQuery connector settings
  bigquery:
    materializationDataset: temp_spark_bq
    viewsEnabled: true

# Logging Settings
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  handlers:
    console:
      enabled: true
      level: INFO
      
    file:
      enabled: true
      level: DEBUG
      path: logs/pipeline.log
      max_bytes: 10485760  # 10MB
      backup_count: 5

# Output Paths
outputs:
  base_path: outputs
  
  directories:
    data_dictionary: data_dictionary
    quality_reports: quality_reports
    model_artifacts: model_artifacts
    evaluation_reports: evaluation_reports
    feature_store: feature_store

# Pipeline Settings
pipeline:
  # Random seed for reproducibility
  random_state: 42
  
  # Date format for parsing
  date_format: "%Y-%m-%d"
  
  # Maximum rows before sampling (for Spark -> Pandas conversion)
  max_rows_for_pandas: 1000000
